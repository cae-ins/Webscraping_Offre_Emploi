{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31df4115",
   "metadata": {},
   "source": [
    "# Educarriere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54348fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "# Fonction pour extraire le texte d'un élément HTML\n",
    "def extract_text(element, class_name=None, style=None, text_contains=None):\n",
    "    if element:\n",
    "        tag = element.find(class_=class_name, style=style, text=text_contains)\n",
    "        return tag.text.strip() if tag else \"\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Fonction pour nettoyer le texte\n",
    "def clean_text(text):\n",
    "    if text is not None:\n",
    "        cleaned_text = text.replace('D\\x92', ' ').replace('d\\x92', ' ').replace('\\x92', ' ').replace('\\r\\n', '').replace('\\xa0', '')\n",
    "        return cleaned_text.strip() if cleaned_text else None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Fonction pour extraire la date à partir d'un élément HTML\n",
    "def extract_date(element, text_contains):\n",
    "    date_elements = element.find_all('a', class_='text')\n",
    "    date = next((e.find('span', style='color:#FF0000;').text.strip() for e in date_elements if text_contains in e.text), \"\")\n",
    "    return date\n",
    "\n",
    "# Fonction pour extraire les détails des offres d'emploi\n",
    "def scrap_job_details(urls):\n",
    "    # En-tête pour éviter d'être bloqué\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Pour exécuter le navigateur en arrière-plan\n",
    "    options.add_argument(\"--disable-gpu\")  # Désactiver l'accélération GPU en mode headless\n",
    "    chrome_driver_path = \"C:\\\\Users\\\\ngora\\\\OneDrive\\\\Bureau\\\\INS_DATA\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"  # Remplacez par l'emplacement réel de votre Chrome binary\n",
    "    options.add_argument(f\"webdriver.chrome.driver={chrome_driver_path}\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Liste pour stocker les détails de chaque emploi\n",
    "    all_job_details = []\n",
    "\n",
    "    # Parcourir les liens\n",
    "    for url in urls:\n",
    "        req = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        time.sleep(5)  # Attendre 5 secondes avant la prochaine requête\n",
    "\n",
    "        offres = soup.find_all('div', class_='box row')\n",
    "\n",
    "        # Parcourir les offres d'emploi sur la page principale\n",
    "        for offre in offres:\n",
    "            # Trouver la balise <h4> dans la structure HTML pour extraire le lien\n",
    "            offre_link_tag = offre.find('h4')\n",
    "\n",
    "            # Vérifier si la balise <h4> a été trouvée\n",
    "            if offre_link_tag:\n",
    "                # Extraire le lien de l'attribut 'href'\n",
    "                offre_link = offre_link_tag.find('a')['href']\n",
    "                all_job_details.append({'Offre_Link': offre_link})\n",
    "\n",
    "    # Fermer le pilote Selenium à la fin\n",
    "    driver.quit()\n",
    "\n",
    "    # Concaténer tous les détails des emplois en un seul DataFrame\n",
    "    if all_job_details:\n",
    "        all_job_details_df = pd.DataFrame(all_job_details)\n",
    "        return all_job_details_df\n",
    "    else:\n",
    "        print(\"Aucun détail d'offre d'emploi trouvé.\")\n",
    "        return None\n",
    "\n",
    "# Fonction pour récupérer les données des offres d'emploi\n",
    "def scrape_emploi_ci(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=200)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur de connexion à {url} : {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    job_description_wrappers = soup.find_all('div', class_='box row')\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for wrapper in job_description_wrappers:\n",
    "        h4_tag = wrapper.find('h4')\n",
    "        poste = extract_text(h4_tag)\n",
    "\n",
    "        entry_title_tag = wrapper.find('p', class_='entry-title')\n",
    "        sous_titre = extract_text(entry_title_tag)\n",
    "\n",
    "        a_text_tag = wrapper.find('a', class_='text')\n",
    "        code = extract_text(a_text_tag, style='color:#FF0000;')\n",
    "\n",
    "        date_edition = extract_date(wrapper, \"Date d'édition:\")\n",
    "        date_limite = extract_date(wrapper, \"Date limite:\")\n",
    "\n",
    "        pays_tag = wrapper.find('a', class_='text')\n",
    "        pays = pays_tag.find_parent().text.strip().split()[-1] if pays_tag else None\n",
    "\n",
    "        sous_titre = clean_text(sous_titre)\n",
    "\n",
    "        data_list.append({\n",
    "            'Poste': clean_text(poste),\n",
    "            'Sous_titre': sous_titre,\n",
    "            'Code': clean_text(code),\n",
    "            'Date_DEdition': date_edition,\n",
    "            'Date_limite': date_limite,\n",
    "            'Pays': clean_text(pays),\n",
    "            'URL': url  # Ajout de la colonne 'URL'\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "# Fonction pour ajouter la colonne 'Offre_Link' à un DataFrame existant\n",
    "def add_offre_link_column(result_df):\n",
    "    job_details_df = scrap_job_details(list(result_df[\"URL\"]))\n",
    "    if job_details_df is not None:\n",
    "        # Join des DataFrames\n",
    "        result_df = pd.concat([result_df, job_details_df], axis=1)\n",
    "        return result_df\n",
    "    else:\n",
    "        print(\"Impossible d'ajouter la colonne 'Offre_Link' au DataFrame.\")\n",
    "        return None\n",
    "\n",
    "# Liste des liens\n",
    "urls = [\"https://emploi.educarriere.ci/nos-offres?page1={}&codes=&mots_cles=&typeemploi1=&niveau1=&anciennete=&typeoffre1=&recruteur=\".format(category) for category in range(40)]\n",
    "\n",
    "# Créer un DataFrame à partir des liens\n",
    "result_df = pd.concat([scrape_emploi_ci(url) for url in urls], ignore_index=True)\n",
    "\n",
    "# Supprimer les lignes dont toutes les variables n'ont pas de données\n",
    "#result_df = result_df.dropna(how='any').reset_index(drop=True)\n",
    "\n",
    "# Ajouter la colonne 'Offre_Link'\n",
    "result_df = add_offre_link_column(result_df)\n",
    "# Supprimer les lignes avec plus de 80% de valeurs NaN\n",
    "threshold = int(result_df.shape[1] * 0.8)  # 80% des colonnes\n",
    "result_df = result_df.dropna(thresh=threshold)\n",
    "# Réordonner les index\n",
    "result_df = result_df.reset_index(drop=True)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import time\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "\n",
    "# (votre fonction extract_job_information reste inchangée)\n",
    "def extract_job_information(soup, url):\n",
    "    try:\n",
    "        # Extraction des informations de l'offre d'emploi\n",
    "        poste = soup.select_one('li.list-group-item:-soup-contains(\"Poste\")').strong.next_sibling.strip()\n",
    "        type_offre = soup.select_one('li.list-group-item:-soup-contains(\"Type d\\'offre\")').strong.next_sibling.strip()\n",
    "        metiers = soup.select_one('li.list-group-item:-soup-contains(\"Métier(s):\")').strong.next_sibling.strip()\n",
    "        niveaux = soup.select_one('li.list-group-item:-soup-contains(\"Niveau(x):\")').strong.next_sibling.strip()\n",
    "        experience = soup.select_one('li.list-group-item:-soup-contains(\"Expérience:\")').strong.next_sibling.strip()\n",
    "        lieu = soup.select_one('li.list-group-item:-soup-contains(\"Lieu:\")').strong.next_sibling.strip()\n",
    "        \n",
    "        # Extraction des dates de publication et de limite\n",
    "        date_publication = soup.find('strong', string='Date de publication:').find_next('span').text.strip()\n",
    "        date_limite = soup.find('strong', string='Date limite:').find_next('span').text.strip()\n",
    "        \n",
    "        # description = soup.select_one('div.text-col.post.small-post.col-md-9.col-xs-12 ul.list-group').text.strip()\n",
    "        description = soup.select_one('div.entry-content').text.strip()\n",
    "\n",
    "        return {\n",
    "            \"Poste\": [poste],\n",
    "            \"Type d'offre\": [type_offre],\n",
    "            \"Métier(s)\": [metiers],\n",
    "            \"Niveau(x)\": [niveaux],\n",
    "            \"Expérience\": [experience],\n",
    "            \"Lieu\": [lieu],\n",
    "            \"URL\": [url],\n",
    "            \"Date de publication\": [date_publication],\n",
    "            \"Date limite\": [date_limite],\n",
    "            \"Description\": [description]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting job information for URL {url}: {e}\")\n",
    "        # Retourner un dictionnaire avec l'URL en cas d'erreur\n",
    "        return {\"URL\": [url]}\n",
    "\n",
    "# Liste des URLs à scraper\n",
    "urls = list(result_df['Offre_Link'])\n",
    "\n",
    "# Liste pour stocker les DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Boucle sur chaque URL\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Envoyer une requête GET au site avec un délai de 120 secondes\n",
    "        response = requests.get(url, headers=headers, verify=True, timeout=120)\n",
    "\n",
    "        # Vérifier si la requête a réussi (statut 200)\n",
    "        if response.status_code == 200:\n",
    "            # Analyser le contenu de la page avec BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extraire les informations sur l'emploi\n",
    "            job_info = extract_job_information(soup, url)\n",
    "\n",
    "            # Créer un DataFrame\n",
    "            df = pd.DataFrame(job_info)\n",
    "\n",
    "            # Ajouter le DataFrame à la liste\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(f\"Échec de la requête pour l'URL {url}. Statut : {response.status_code}\")\n",
    "            # Ajouter une ligne avec l'URL en cas d'erreur\n",
    "            dfs.append(pd.DataFrame({\"URL\": [url]}))\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout lors de la requête pour l'URL {url}\")\n",
    "        # Ajouter une ligne avec l'URL en cas d'erreur\n",
    "        dfs.append(pd.DataFrame({\"URL\": [url]}))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Une erreur s'est produite lors de la requête pour l'URL {url}: {e}\")\n",
    "        # Ajouter une ligne avec l'URL en cas d'erreur\n",
    "        dfs.append(pd.DataFrame({\"URL\": [url]}))\n",
    "\n",
    "    # Ajouter un délai de 5 secondes entre les requêtes pour éviter d'être bloqué\n",
    "    time.sleep(5)\n",
    "\n",
    "# Concaténer tous les DataFrames en un seul DataFrame\n",
    "df_Educarriere = pd.concat(dfs, ignore_index=True)\n",
    "df_Educarriere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1268703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import time\n",
    "\n",
    "# Désactiver les avertissements liés aux requêtes non sécurisées\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# (votre fonction extract_job_information reste inchangée)\n",
    "def extract_job_information(soup):\n",
    "    # Extraction des informations de l'offre d'emploi\n",
    "    poste = soup.select_one('li.list-group-item:-soup-contains(\"Poste\")').strong.next_sibling.strip()\n",
    "    type_offre = soup.select_one('li.list-group-item:-soup-contains(\"Type d\\'offre\")').strong.next_sibling.strip()\n",
    "    metiers = soup.select_one('li.list-group-item:-soup-contains(\"Métier(s):\")').strong.next_sibling.strip()\n",
    "    niveaux = soup.select_one('li.list-group-item:-soup-contains(\"Niveau(x):\")').strong.next_sibling.strip()\n",
    "    experience = soup.select_one('li.list-group-item:-soup-contains(\"Expérience:\")').strong.next_sibling.strip()\n",
    "    lieu = soup.select_one('li.list-group-item:-soup-contains(\"Lieu:\")').strong.next_sibling.strip()\n",
    "    \n",
    "    # Extraction des dates de publication et de limite\n",
    "    date_publication = soup.find('strong', string='Date de publication:').find_next('span').text.strip()\n",
    "    date_limite = soup.find('strong', string='Date limite:').find_next('span').text.strip()\n",
    "    \n",
    "   # description = soup.select_one('div.text-col.post.small-post.col-md-9.col-xs-12 ul.list-group').text.strip()\n",
    "    description = soup.select_one('div.entry-content').text.strip()\n",
    "\n",
    "    return {\n",
    "        \"Poste\": [poste],\n",
    "        \"Type d'offre\": [type_offre],\n",
    "        \"Métier(s)\": [metiers],\n",
    "        \"Niveau(x)\": [niveaux],\n",
    "        \"Expérience\": [experience],\n",
    "        \"Lieu\": [lieu],\n",
    "        \"Date de publication\": [date_publication],\n",
    "        \"Date limite\": [date_limite],\n",
    "        \"Description\": [description]\n",
    "    }\n",
    "\n",
    "# Liste des URLs à scraper\n",
    "urls = list(all_job_details_df['Offre_Link'])\n",
    "\n",
    "# Liste pour stocker les DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Boucle sur chaque URL\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Envoyer une requête GET au site avec un délai de 120 secondes\n",
    "        response = requests.get(url, headers=headers, verify=True, timeout=120)\n",
    "\n",
    "        # Vérifier si la requête a réussi (statut 200)\n",
    "        if response.status_code == 200:\n",
    "            # Analyser le contenu de la page avec BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                # Extraire les informations sur l'emploi\n",
    "                job_info = extract_job_information(soup)\n",
    "                job_info['URL'] = [url]\n",
    "\n",
    "                # Créer un DataFrame\n",
    "                df = pd.DataFrame(job_info)\n",
    "\n",
    "                # Ajouter le DataFrame à la liste\n",
    "                dfs.append(df)\n",
    "\n",
    "            except Exception as e:\n",
    "                df = pd.DataFrame({\"URL\": [url]})\n",
    "                 \n",
    "                # Ajouter le DataFrame à la liste\n",
    "                dfs.append(df)\n",
    "                print(f\"An error occurred while extracting job information: {e}\")\n",
    "        else:\n",
    "            df = pd.DataFrame({\"URL\": [url]})\n",
    "            dfs.append(df)\n",
    "            print(f\"Échec de la requête pour l'URL {url}. Statut : {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        df = pd.DataFrame({\"URL\": [url]})\n",
    "        dfs.append(df)\n",
    "        print(f\"Timeout lors de la requête pour l'URL {url}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        df = pd.DataFrame({\"URL\": [url]})\n",
    "        dfs.append(df)\n",
    "        print(f\"Une erreur s'est produite lors de la requête pour l'URL {url}: {e}\")\n",
    "\n",
    "    # Ajouter un délai de 5 secondes entre les requêtes pour éviter d'être bloqué\n",
    "    time.sleep(5)\n",
    "\n",
    "# Concaténer tous les DataFrames en un seul DataFrame\n",
    "df_Educarriere = pd.concat(dfs, ignore_index=True)\n",
    "df_Educarriere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a877cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0ee1833",
   "metadata": {},
   "source": [
    "# Novojob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "113f9f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intitule</th>\n",
       "      <th>Entreprise</th>\n",
       "      <th>Pays</th>\n",
       "      <th>Date</th>\n",
       "      <th>Niveau</th>\n",
       "      <th>Experience_lettre</th>\n",
       "      <th>url</th>\n",
       "      <th>Offre_Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Livreurs Moto</td>\n",
       "      <td>Entreprise anonyme</td>\n",
       "      <td>Côte d'ivoire</td>\n",
       "      <td>04 Janvier</td>\n",
       "      <td>Confirmé / Expérimenté</td>\n",
       "      <td>Sans expérience</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pompiste H/F</td>\n",
       "      <td>None</td>\n",
       "      <td>Abidjan, Côte d'ivoire</td>\n",
       "      <td>27 Décembre 2023</td>\n",
       "      <td>Débutant / Junior</td>\n",
       "      <td>Moins d’un an</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caissier H/F</td>\n",
       "      <td>Coris consulting</td>\n",
       "      <td>Côte d'ivoire</td>\n",
       "      <td>27 Décembre 2023</td>\n",
       "      <td>Débutant / Junior</td>\n",
       "      <td>Sans expérience</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ouvrier de rayon</td>\n",
       "      <td>Coris consulting</td>\n",
       "      <td>Côte d'ivoire</td>\n",
       "      <td>27 Décembre 2023</td>\n",
       "      <td>Débutant / Junior</td>\n",
       "      <td>Sans expérience</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Commercial Terrain</td>\n",
       "      <td>KOMIAN AI</td>\n",
       "      <td>Côte d'ivoire</td>\n",
       "      <td>02 Février</td>\n",
       "      <td>Débutant / Junior</td>\n",
       "      <td>Moins d’un an</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>Responsable Clientèle Patrimoniale (H/F)</td>\n",
       "      <td>Société Ivoirienne de Banque (SIB)</td>\n",
       "      <td>Abidjan, Côte d'ivoire</td>\n",
       "      <td>05 Février</td>\n",
       "      <td>Confirmé / Expérimenté</td>\n",
       "      <td>3 à 5 ans</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>Technicien Génie Civil Option Bâtiment</td>\n",
       "      <td>SOCOPI</td>\n",
       "      <td>Côte d'ivoire</td>\n",
       "      <td>17 Janvier</td>\n",
       "      <td>Débutant / Junior</td>\n",
       "      <td>1 à 2 ans</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>Chargé de Support Système Windows et Virtualis...</td>\n",
       "      <td>Ascens</td>\n",
       "      <td>Abidjan, Côte d'ivoire</td>\n",
       "      <td>24 Novembre 2023</td>\n",
       "      <td>Confirmé / Expérimenté</td>\n",
       "      <td>6 à 10 ans</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>Chargé.e de Dossiers Achats et Contrats (Assis...</td>\n",
       "      <td>Giz Côte d'Ivoire</td>\n",
       "      <td>Korhogo, Côte d'ivoire</td>\n",
       "      <td>01 Février</td>\n",
       "      <td>Débutant / Junior</td>\n",
       "      <td>1 à 2 ans</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>Field Coordinator(Coordinateur Terrain)</td>\n",
       "      <td>MyAgro CI</td>\n",
       "      <td>Côte d'ivoire...</td>\n",
       "      <td>10 Décembre 2023</td>\n",
       "      <td>Confirmé / Expérimenté</td>\n",
       "      <td>3 à 5 ans</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "      <td>https://www.novojob.com/cote-d-ivoire/offres-d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>711 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Intitule  \\\n",
       "0                                        Livreurs Moto   \n",
       "1                                         Pompiste H/F   \n",
       "2                                         Caissier H/F   \n",
       "3                                     Ouvrier de rayon   \n",
       "4                                   Commercial Terrain   \n",
       "..                                                 ...   \n",
       "706           Responsable Clientèle Patrimoniale (H/F)   \n",
       "707             Technicien Génie Civil Option Bâtiment   \n",
       "708  Chargé de Support Système Windows et Virtualis...   \n",
       "709  Chargé.e de Dossiers Achats et Contrats (Assis...   \n",
       "710            Field Coordinator(Coordinateur Terrain)   \n",
       "\n",
       "                             Entreprise                    Pays  \\\n",
       "0                    Entreprise anonyme           Côte d'ivoire   \n",
       "1                                  None  Abidjan, Côte d'ivoire   \n",
       "2                      Coris consulting           Côte d'ivoire   \n",
       "3                      Coris consulting           Côte d'ivoire   \n",
       "4                             KOMIAN AI           Côte d'ivoire   \n",
       "..                                  ...                     ...   \n",
       "706  Société Ivoirienne de Banque (SIB)  Abidjan, Côte d'ivoire   \n",
       "707                              SOCOPI           Côte d'ivoire   \n",
       "708                              Ascens  Abidjan, Côte d'ivoire   \n",
       "709                   Giz Côte d'Ivoire  Korhogo, Côte d'ivoire   \n",
       "710                           MyAgro CI        Côte d'ivoire...   \n",
       "\n",
       "                 Date                  Niveau Experience_lettre  \\\n",
       "0          04 Janvier  Confirmé / Expérimenté   Sans expérience   \n",
       "1    27 Décembre 2023       Débutant / Junior     Moins d’un an   \n",
       "2    27 Décembre 2023       Débutant / Junior   Sans expérience   \n",
       "3    27 Décembre 2023       Débutant / Junior   Sans expérience   \n",
       "4          02 Février       Débutant / Junior     Moins d’un an   \n",
       "..                ...                     ...               ...   \n",
       "706        05 Février  Confirmé / Expérimenté         3 à 5 ans   \n",
       "707        17 Janvier       Débutant / Junior         1 à 2 ans   \n",
       "708  24 Novembre 2023  Confirmé / Expérimenté        6 à 10 ans   \n",
       "709        01 Février       Débutant / Junior         1 à 2 ans   \n",
       "710  10 Décembre 2023  Confirmé / Expérimenté         3 à 5 ans   \n",
       "\n",
       "                                                   url  \\\n",
       "0    https://www.novojob.com/cote-d-ivoire/offres-d...   \n",
       "1    https://www.novojob.com/cote-d-ivoire/offres-d...   \n",
       "2    https://www.novojob.com/cote-d-ivoire/offres-d...   \n",
       "3    https://www.novojob.com/cote-d-ivoire/offres-d...   \n",
       "4    https://www.novojob.com/cote-d-ivoire/offres-d...   \n",
       "..                                                 ...   \n",
       "706  https://www.novojob.com/cote-d-ivoire/offres-d...   \n",
       "707  https://www.novojob.com/cote-d-ivoire/offres-d...   \n",
       "708  https://www.novojob.com/cote-d-ivoire/offres-d...   \n",
       "709  https://www.novojob.com/cote-d-ivoire/offres-d...   \n",
       "710  https://www.novojob.com/cote-d-ivoire/offres-d...   \n",
       "\n",
       "                                            Offre_Link  \n",
       "0    https://www.novojob.com/cote-d-ivoire/offres-d...  \n",
       "1    https://www.novojob.com/cote-d-ivoire/offres-d...  \n",
       "2    https://www.novojob.com/cote-d-ivoire/offres-d...  \n",
       "3    https://www.novojob.com/cote-d-ivoire/offres-d...  \n",
       "4    https://www.novojob.com/cote-d-ivoire/offres-d...  \n",
       "..                                                 ...  \n",
       "706  https://www.novojob.com/cote-d-ivoire/offres-d...  \n",
       "707  https://www.novojob.com/cote-d-ivoire/offres-d...  \n",
       "708  https://www.novojob.com/cote-d-ivoire/offres-d...  \n",
       "709  https://www.novojob.com/cote-d-ivoire/offres-d...  \n",
       "710  https://www.novojob.com/cote-d-ivoire/offres-d...  \n",
       "\n",
       "[711 rows x 8 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Liste des liens pour chaque catégorie\n",
    "categories = [\n",
    "    \"toutes les offres d'emploi\",\n",
    "    \"juridique,fiscal,audit,conseil\",\n",
    "    \"administrations,moyens généraux\",\n",
    "    \"assistanat,secrétariat\",\n",
    "    \"metiers banque et assurances\",\n",
    "    \"RH,personnel,formation\",\n",
    "    \"education,enseignement\",\n",
    "    \"direction générale,direction d'unité\",\n",
    "    \"vente,televente,assistanat\",\n",
    "    \"commercial,technico commercial,service client\",\n",
    "    \"responsable commercial,grands comptes\",\n",
    "    \"créatio, design\",\n",
    "    \"marketing, communication\",\n",
    "    \"journalisme,médias,traduction\",\n",
    "    \"informatique,systèmes d'information,internet\",\n",
    "    \"télécommunication,réseaux\",\n",
    "    \"chantier,métiers BTP,architecture\",\n",
    "    \"ingénierie,etudes,projet,R&D\",\n",
    "    \"logistique,achat,stock,transport\",\n",
    "    \"production,méthode,industrie\",\n",
    "    \"maintenance,entretien\",\n",
    "    \"Qualité,sécurité,Environnement\",\n",
    "    \"Santé,Médical,Pharmacie\",\n",
    "    \"Hotellerie,Tourisme,Restauration, Loisirs\",\n",
    "    \"Ouvriers qualifiés, Chauffeur\",\n",
    "    \"autre\",\n",
    "    \"Métiers de l'agriculture\"\n",
    "]\n",
    "\n",
    "base_url = \"https://www.novojob.com/cote-d-ivoire/offres-d-emploi?q=\"\n",
    "category_links = [f\"{base_url}{'+'.join(category.split(','))}\" for category in categories]\n",
    "\n",
    "# Utilisation d'un en-tête pour éviter d'être bloqué\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "intitules_list = []\n",
    "entreprises_list = []\n",
    "pays_list = []\n",
    "dates_list = []\n",
    "niveau_list = []\n",
    "experience_list = []\n",
    "lien_list = []\n",
    "all_job_lien = []\n",
    "\n",
    "# Parcourir les liens de chaque catégorie\n",
    "for category_link in category_links:\n",
    "    req = requests.get(category_link, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    time.sleep(5)  # Attendre 5 secondes avant la prochaine requête\n",
    "\n",
    "    offres = soup.find_all('div', class_='row-fluid job-details pointer')\n",
    "\n",
    "    for offre in offres:\n",
    "        offre_link_tag = offre.find('a')\n",
    "        if offre_link_tag:\n",
    "            offre_link = offre_link_tag['href']\n",
    "            all_job_lien.append(offre_link)\n",
    "\n",
    "        entreprise_tag = offre.find('h6', class_='ellipsis')\n",
    "        entreprise = entreprise_tag.get_text().strip() if entreprise_tag else None\n",
    "\n",
    "        intitule_tag = offre.find('h2', class_='ellipsis row-fluid')\n",
    "        intitule = intitule_tag.get_text().strip() if intitule_tag else None\n",
    "\n",
    "        bloc_bottom = offre.find_next('div', class_='bloc-bottom')\n",
    "\n",
    "        pays_info = bloc_bottom.find('i', class_='fa fa-map-marker icon-left')\n",
    "        pays = pays_info.find_parent().text.strip() if pays_info else None\n",
    "\n",
    "        date_info = bloc_bottom.find('i', class_='fa fa-clock-o icon-left')\n",
    "        date = date_info.find_parent().text.strip() if date_info else None\n",
    "\n",
    "        niveau_info = bloc_bottom.find('i', class_='fa fa-bookmark icon-left')\n",
    "        niveau_text = niveau_info.find_parent().text.strip() if niveau_info else None\n",
    "\n",
    "        match = re.match(r'(.+) \\((.+)\\)', niveau_text)\n",
    "        niveau_col, experience_col = match.groups() if match else (None, None)\n",
    "\n",
    "        intitules_list.append(intitule)\n",
    "        entreprises_list.append(entreprise)\n",
    "        pays_list.append(pays)\n",
    "        dates_list.append(date)\n",
    "        niveau_list.append(niveau_col)\n",
    "        experience_list.append(experience_col)\n",
    "        url_list.append(category_link)\n",
    "\n",
    "# Création du DataFrame\n",
    "df_offers = pd.DataFrame({\n",
    "    'Intitule': intitules_list,\n",
    "    'Entreprise': entreprises_list,\n",
    "    'Pays': pays_list,\n",
    "    'Date': dates_list,\n",
    "    'Niveau': niveau_list,\n",
    "    'Experience_lettre': experience_list,\n",
    "    'url': url_list,\n",
    "    'Offre_Link': all_job_lien\n",
    "})\n",
    "\n",
    "df_offers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "839d8b83",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='www.novojob.com', port=443): Read timed out. (read timeout=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1096\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1096\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:642\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    634\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    635\u001b[0m         (\n\u001b[0;32m    636\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem time is way off (before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRECENT_DATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). This will probably \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    639\u001b[0m         SystemTimeWarning,\n\u001b[0;32m    640\u001b[0m     )\n\u001b[1;32m--> 642\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[0;32m    643\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    644\u001b[0m     cert_reqs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_reqs,\n\u001b[0;32m    645\u001b[0m     ssl_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version,\n\u001b[0;32m    646\u001b[0m     ssl_minimum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_minimum_version,\n\u001b[0;32m    647\u001b[0m     ssl_maximum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_maximum_version,\n\u001b[0;32m    648\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[0;32m    649\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[0;32m    650\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[0;32m    651\u001b[0m     cert_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[0;32m    652\u001b[0m     key_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[0;32m    653\u001b[0m     key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[0;32m    654\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    655\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context,\n\u001b[0;32m    656\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[0;32m    657\u001b[0m     assert_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_hostname,\n\u001b[0;32m    658\u001b[0m     assert_fingerprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_fingerprint,\n\u001b[0;32m    659\u001b[0m )\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:782\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    780\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 782\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    783\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    784\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39mkey_file,\n\u001b[0;32m    785\u001b[0m     certfile\u001b[38;5;241m=\u001b[39mcert_file,\n\u001b[0;32m    786\u001b[0m     key_password\u001b[38;5;241m=\u001b[39mkey_password,\n\u001b[0;32m    787\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39mca_certs,\n\u001b[0;32m    788\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39mca_cert_dir,\n\u001b[0;32m    789\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39mca_cert_data,\n\u001b[0;32m    790\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    791\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    792\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[0;32m    793\u001b[0m )\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\ssl_.py:470\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\ssl_.py:514\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    524\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1108\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1379\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] Une tentative de connexion a échoué car le parti connecté n’a pas répondu convenablement au-delà d’une certaine durée ou une connexion établie a échoué car l’hôte de connexion n’a pas répondu",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    842\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 844\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    845\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    846\u001b[0m )\n\u001b[0;32m    847\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:470\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m reraise(\u001b[38;5;28mtype\u001b[39m(error), error, _stacktrace)\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:469\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:370\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[1;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='www.novojob.com', port=443): Read timed out. (read timeout=None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m job_urls:\n\u001b[0;32m     10\u001b[0m     headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m     }\n\u001b[1;32m---> 14\u001b[0m     req \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m     15\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(req\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     job_details \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:532\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[1;32m--> 532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='www.novojob.com', port=443): Read timed out. (read timeout=None)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Example usage for multiple job URLs\n",
    "job_urls = list(df_offers['Offre_Link'])\n",
    "\n",
    "# Extract details for each job URL\n",
    "all_job_details = []\n",
    "for url in job_urls:\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    job_details = {}\n",
    "    \n",
    "    # Ajouter le lien\n",
    "    job_details[\"Offre_Link\"]=url\n",
    "    # Extracting job details\n",
    "    details_section = soup.find('ul', class_='text-small')\n",
    "    if details_section:\n",
    "        for li in details_section.find_all('li', class_='row-fluid'):\n",
    "            key = li.find('span', class_='span4').text.strip()\n",
    "            value = li.find('span', class_='span8').text.strip()\n",
    "            job_details[key] = value\n",
    "\n",
    "    # Extracting the provided text\n",
    "    description_section = soup.find('div', class_='spaced details-description')\n",
    "    if description_section:\n",
    "        provided_text = description_section.text.strip()\n",
    "        job_details['Provided Text'] = provided_text\n",
    "\n",
    "    all_job_details.append(job_details)\n",
    "\n",
    "# Create a DataFrame\n",
    "df_Novojob = pd.DataFrame(all_job_details)\n",
    "df_Novojob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998eea12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e79cfeca",
   "metadata": {},
   "source": [
    "# Emploi.ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3161eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text(element, tag_name=None):\n",
    "    tag = element.find(tag_name)\n",
    "    return tag.text.strip() if tag else \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace('D\\x92', ' ').replace('d\\x92', ' ').replace('\\x92', ' ').replace('\\r\\n', '').replace('\\xa0', '')\n",
    "\n",
    "def scrape_emploi_ci(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=500)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur de connexion à {url} : {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    job_description_wrappers = soup.find_all('div', class_='job-description-wrapper')\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for wrapper in job_description_wrappers:\n",
    "        h5_tag = wrapper.find('h5')\n",
    "        poste = extract_text(h5_tag, 'a')\n",
    "\n",
    "        job_recruiter_tag = wrapper.find('p', class_='job-recruiter')\n",
    "        date_and_company = job_recruiter_tag.text.strip().split('|')\n",
    "        date = date_and_company[0].strip() if date_and_company else \"\"\n",
    "        entreprise = extract_text(job_recruiter_tag, 'a')\n",
    "\n",
    "        description_tag = wrapper.find('div', class_='search-description')\n",
    "        description = clean_text(description_tag.text.strip()) if description_tag else \"\"\n",
    "\n",
    "        region_tag = wrapper.find('p', text='Région de :')\n",
    "        region = extract_text(region_tag) if region_tag else \"\"\n",
    "\n",
    "        data_list.append({\n",
    "            'Poste': poste,\n",
    "            'Entreprise': entreprise,\n",
    "            'Date': date,\n",
    "            'Description': description,\n",
    "            'Région': region,\n",
    "            \"URL\":url\n",
    "\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fonction pour extraire les détails des offres d'emploi\n",
    "def scrap_job_details(urls):\n",
    "    # En-tête pour éviter d'être bloqué\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Pour exécuter le navigateur en arrière-plan\n",
    "    options.add_argument(\"--disable-gpu\")  # Désactiver l'accélération GPU en mode headless\n",
    "    chrome_driver_path = \"C:\\\\Users\\\\ngora\\\\OneDrive\\\\Bureau\\\\INS_DATA\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"  # Remplacez par l'emplacement réel de votre Chrome binary\n",
    "    options.add_argument(f\"webdriver.chrome.driver={chrome_driver_path}\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Liste pour stocker les détails de chaque emploi\n",
    "    all_job_details = []\n",
    "\n",
    "    # Parcourir les liens\n",
    "    for url in urls:\n",
    "        req = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        time.sleep(5)  # Attendre 5 secondes avant la prochaine requête\n",
    "\n",
    "        offres = soup.find_all('div', class_=\"job-description-wrapper\")\n",
    "\n",
    "        # Parcourir les offres d'emploi sur la page principale\n",
    "        for offre in offres:\n",
    "            # Trouver la balise <h4> dans la structure HTML pour extraire le lien\n",
    "            offre_link_tag = offre.find('h5')\n",
    "\n",
    "            # Vérifier si la balise <h4> a été trouvée\n",
    "            if offre_link_tag:\n",
    "                # Extraire le lien de l'attribut 'href'\n",
    "                offre_link = offre_link_tag.find('a')['href']\n",
    "                all_job_details.append({'Offre_Link': \"https://www.emploi.ci\"+offre_link})\n",
    "\n",
    "    # Fermer le pilote Selenium à la fin\n",
    "    driver.quit()\n",
    "\n",
    "    # Concaténer tous les détails des emplois en un seul DataFrame\n",
    "    if all_job_details:\n",
    "        all_job_details_df = pd.DataFrame(all_job_details)\n",
    "        return all_job_details_df\n",
    "    else:\n",
    "        print(\"Aucun détail d'offre d'emploi trouvé.\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "# Fonction pour ajouter la colonne 'Offre_Link' à un DataFrame existant\n",
    "def add_offre_link_column(emploi_df):\n",
    "    job_details_df = scrap_job_details(list(emploi_df[\"URL\"]))\n",
    "    if job_details_df is not None:\n",
    "        # Join des DataFrames\n",
    "        result_df = pd.concat([emploi_df, job_details_df], axis=1)\n",
    "        return emploi_df\n",
    "    else:\n",
    "        print(\"Impossible d'ajouter la colonne 'Offre_Link' au DataFrame.\")\n",
    "        return None\n",
    "\n",
    "# Liste des liens\n",
    "categories = [\"31\", \"1127\", \"29\", \"37\", \"1115\", \"30\", \"1115\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"39\", \"38\", \"40\", \"525\", \"41\", \"28\"]\n",
    "#categories=[\"31\"]\n",
    "# Liste d'URLs générées\n",
    "urls = [\"https://www.emploi.ci/recherche-jobs-cote-ivoire/?f%5B0%5D=im_field_offre_metiers%3A{}\".format(category) for category in categories]\n",
    "\n",
    "# Créer un DataFrame à partir des liens\n",
    "emploi_df = pd.concat([scrape_emploi_ci(url) for url in urls], ignore_index=True)\n",
    "\n",
    "# Supprimer les lignes dont toutes les variables n'ont pas de données\n",
    "#result_df = result_df.dropna(how='any').reset_index(drop=True)\n",
    "\n",
    "# Ajouter la colonne 'Offre_Link'\n",
    "emploi_df = add_offre_link_column(emploi_df)\n",
    "# Supprimer les lignes avec plus de 80% de valeurs NaN\n",
    "threshold = int(emploi_df.shape[1] * 0.8)  # 80% des colonnes\n",
    "emploi_df = emploi_df.dropna(thresh=threshold)\n",
    "emploi_df=emploi_df.reset_index(drop=True)\n",
    "# Afficher le DataFrame\n",
    "emploi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dcd0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
