{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370ab4d9",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"image.png\" alt=\"Logo du projet\" style=\"width: 150px; height: auto; border-radius: 8px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);\">\n",
    "</p>\n",
    "\n",
    "<h1 align=\"center\" style=\"color: #FFA500; font-size: 2.5em; font-weight: bold;\">Projet INS_PHAS</h1>\n",
    "\n",
    "<p align=\"center\" style=\"color: #4CAF50; font-size: 1.2em;\">\n",
    "  Analyse des offres d’emploi au moyen des données scrapées.\n",
    "</p>\n",
    "\n",
    "<p align=\"center\" style=\"color: #4CAF50; font-size: 1.1em;\">\n",
    "  <strong>Auteur :</strong> DOUMBIA ABDOULAYE (<a href=\"mailto:abdoulaye.doumbi19@inphb.ci\" style=\"color: #FFA500;\">abdoulaye.doumbi19@inphb.ci</a>)\n",
    "</p>\n",
    "\n",
    "<p align=\"center\" style=\"color: #4CAF50; font-size: 1.1em;\">\n",
    "  Ce matériel est soumis aux termes et conditions de la licence <a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" style=\"color: #FFA500;\">Creative Commons CC BY-NC-SA 4.0</a>. L'utilisation gratuite est autorisée à des fins non commerciales.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7171af53",
   "metadata": {},
   "source": [
    "# Educarriere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5dca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur de connexion à https://emploi.educarriere.ci/nos-offres?page1=3&codes=&mots_cles=&typeemploi1=&niveau1=&anciennete=&typeoffre1=&recruteur= : HTTPSConnectionPool(host='emploi.educarriere.ci', port=443): Read timed out.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Poste</th>\n",
       "      <th>Sous_titre</th>\n",
       "      <th>Code</th>\n",
       "      <th>Date_DEdition</th>\n",
       "      <th>Date_limite</th>\n",
       "      <th>Pays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FORMATEUR EN FINANCES COMPTABILITE</td>\n",
       "      <td>SHINE CORPORATErecruteFORMATEUR EN FINANCES CO...</td>\n",
       "      <td>109494</td>\n",
       "      <td>02/02/2024</td>\n",
       "      <td>12/02/2024</td>\n",
       "      <td>Abidjan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAGASINIER</td>\n",
       "      <td>CONCEPTEUR ET CONSTRUCTEUR DE BATIMENTS GROUPr...</td>\n",
       "      <td>109493</td>\n",
       "      <td>02/02/2024</td>\n",
       "      <td>26/02/2024</td>\n",
       "      <td>Abidjan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RESEAU INFORMATIQUE ET TELECOMMUNICATION</td>\n",
       "      <td>MoonvisionsrecruteRESEAU INFORMATIQUE ET TELEC...</td>\n",
       "      <td>109492</td>\n",
       "      <td>02/02/2024</td>\n",
       "      <td>08/02/2024</td>\n",
       "      <td>8EM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UN.E HOMME/FEMME DE MENAGE</td>\n",
       "      <td>MEDECINS DU MONDErecruteUN.E HOMME/FEMME DE ME...</td>\n",
       "      <td>109491</td>\n",
       "      <td>02/02/2024</td>\n",
       "      <td>07/02/2024</td>\n",
       "      <td>SAN-PEDRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01 CHARGE DE RECRUTEMENT</td>\n",
       "      <td>CI-HBSrecrute01 CHARGE DE RECRUTEMENTDescripti...</td>\n",
       "      <td>109490</td>\n",
       "      <td>02/02/2024</td>\n",
       "      <td>16/02/2024</td>\n",
       "      <td>d'I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>STAGE DE SOUTENANCE (RH-COM; GESCOM; FCGE; RIT...</td>\n",
       "      <td>Description du posteStage de soutenanceNous of...</td>\n",
       "      <td>95492</td>\n",
       "      <td>14/02/2023</td>\n",
       "      <td>27/03/2024</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>ESTHÉTICIENNE - PROTHÉSISTE ONGULAIRE (H/F)</td>\n",
       "      <td>Description du posteNous recherchons des esthé...</td>\n",
       "      <td>94035</td>\n",
       "      <td>16/01/2023</td>\n",
       "      <td>23/04/2025</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>CONSULTANT FORMATEUR EN DROIT</td>\n",
       "      <td>Description du posteNous, cabinet de formation...</td>\n",
       "      <td>86696</td>\n",
       "      <td>27/07/2022</td>\n",
       "      <td>02/03/2024</td>\n",
       "      <td>ABIDJAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>CONSULTANT FORMATEUR SPECIALISE</td>\n",
       "      <td>Description du posteNous, cabinet de formation...</td>\n",
       "      <td>86695</td>\n",
       "      <td>27/07/2022</td>\n",
       "      <td>02/03/2024</td>\n",
       "      <td>ABIDJAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>OFFRE D'EMPLOI SÉCURITÉ PRIVÉE</td>\n",
       "      <td>Description du posteNous recherchons des agent...</td>\n",
       "      <td>86143</td>\n",
       "      <td>15/07/2022</td>\n",
       "      <td>31/12/2025</td>\n",
       "      <td>C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Poste  \\\n",
       "0                   FORMATEUR EN FINANCES COMPTABILITE   \n",
       "1                                           MAGASINIER   \n",
       "2             RESEAU INFORMATIQUE ET TELECOMMUNICATION   \n",
       "3                           UN.E HOMME/FEMME DE MENAGE   \n",
       "4                             01 CHARGE DE RECRUTEMENT   \n",
       "..                                                 ...   \n",
       "521  STAGE DE SOUTENANCE (RH-COM; GESCOM; FCGE; RIT...   \n",
       "522        ESTHÉTICIENNE - PROTHÉSISTE ONGULAIRE (H/F)   \n",
       "523                      CONSULTANT FORMATEUR EN DROIT   \n",
       "524                    CONSULTANT FORMATEUR SPECIALISE   \n",
       "525                     OFFRE D'EMPLOI SÉCURITÉ PRIVÉE   \n",
       "\n",
       "                                            Sous_titre    Code Date_DEdition  \\\n",
       "0    SHINE CORPORATErecruteFORMATEUR EN FINANCES CO...  109494    02/02/2024   \n",
       "1    CONCEPTEUR ET CONSTRUCTEUR DE BATIMENTS GROUPr...  109493    02/02/2024   \n",
       "2    MoonvisionsrecruteRESEAU INFORMATIQUE ET TELEC...  109492    02/02/2024   \n",
       "3    MEDECINS DU MONDErecruteUN.E HOMME/FEMME DE ME...  109491    02/02/2024   \n",
       "4    CI-HBSrecrute01 CHARGE DE RECRUTEMENTDescripti...  109490    02/02/2024   \n",
       "..                                                 ...     ...           ...   \n",
       "521  Description du posteStage de soutenanceNous of...   95492    14/02/2023   \n",
       "522  Description du posteNous recherchons des esthé...   94035    16/01/2023   \n",
       "523  Description du posteNous, cabinet de formation...   86696    27/07/2022   \n",
       "524  Description du posteNous, cabinet de formation...   86695    27/07/2022   \n",
       "525  Description du posteNous recherchons des agent...   86143    15/07/2022   \n",
       "\n",
       "    Date_limite       Pays  \n",
       "0    12/02/2024    Abidjan  \n",
       "1    26/02/2024    Abidjan  \n",
       "2    08/02/2024     8EM...  \n",
       "3    07/02/2024  SAN-PEDRO  \n",
       "4    16/02/2024     d'I...  \n",
       "..          ...        ...  \n",
       "521  27/03/2024        ...  \n",
       "522  23/04/2025        ...  \n",
       "523  02/03/2024    ABIDJAN  \n",
       "524  02/03/2024    ABIDJAN  \n",
       "525  31/12/2025       C...  \n",
       "\n",
       "[526 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text(element, class_name=None, style=None, text_contains=None):\n",
    "    if element:\n",
    "        tag = element.find(class_=class_name, style=style, text=text_contains)\n",
    "        return tag.text.strip() if tag else \"\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    if text is not None:\n",
    "        cleaned_text = text.replace('D\\x92', ' ').replace('d\\x92', ' ').replace('\\x92', ' ').replace('\\r\\n', '').replace('\\xa0', '')\n",
    "        return cleaned_text.strip() if cleaned_text else None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_date(element, text_contains):\n",
    "    date_elements = element.find_all('a', class_='text')\n",
    "    date = next((e.find('span', style='color:#FF0000;').text.strip() for e in date_elements if text_contains in e.text), \"\")\n",
    "    return date\n",
    "\n",
    "def scrape_emploi_ci(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=200)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur de connexion à {url} : {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    job_description_wrappers = soup.find_all('div', class_='box row')\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for wrapper in job_description_wrappers:\n",
    "        h4_tag = wrapper.find('h4')\n",
    "        poste = extract_text(h4_tag)\n",
    "\n",
    "        entry_title_tag = wrapper.find('p', class_='entry-title')\n",
    "        sous_titre = extract_text(entry_title_tag)\n",
    "\n",
    "        a_text_tag = wrapper.find('a', class_='text')\n",
    "        code = extract_text(a_text_tag, style='color:#FF0000;')\n",
    "\n",
    "        date_edition = extract_date(wrapper, \"Date d'édition:\")\n",
    "        date_limite = extract_date(wrapper, \"Date limite:\")\n",
    "\n",
    "        pays_tag = wrapper.find('a', class_='text')\n",
    "        pays = pays_tag.find_parent().text.strip().split()[-1] if pays_tag else None\n",
    "\n",
    "        sous_titre = clean_text(sous_titre)\n",
    "\n",
    "        data_list.append({\n",
    "            'Poste': clean_text(poste),\n",
    "            'Sous_titre': sous_titre,\n",
    "            'Code': clean_text(code),\n",
    "            'Date_DEdition': date_edition,\n",
    "            'Date_limite': date_limite,\n",
    "            'Pays': clean_text(pays)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "# Liste des liens\n",
    "urls = [\"https://emploi.educarriere.ci/nos-offres?page1={}&codes=&mots_cles=&typeemploi1=&niveau1=&anciennete=&typeoffre1=&recruteur=\".format(category) for category in range(40)]\n",
    "\n",
    "# Créer un DataFrame à partir des liens\n",
    "result_df = pd.concat([scrape_emploi_ci(url) for url in urls], ignore_index=True)\n",
    "\n",
    "# Supprimer les lignes dont toutes les variables n'ont pas de données\n",
    "result_df = result_df.dropna(how='any').reset_index(drop=True)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a70178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error sending stats to Plausible: error sending request for url (https://plausible.io/api/event): operation timed out\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# Liste des liens\n",
    "urls = [\"https://emploi.educarriere.ci/nos-offres?page1={}&codes=&mots_cles=&typeemploi1=&niveau1=&anciennete=&typeoffre1=&recruteur=\".format(category) for category in range(40)]\n",
    "\n",
    "# En-tête pour éviter d'être bloqué\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Pour exécuter le navigateur en arrière-plan\n",
    "options.add_argument(\"--disable-gpu\")  # Désactiver l'accélération GPU en mode headless\n",
    "chrome_driver_path = \"C:\\\\Users\\\\Dell\\\\Desktop\\\\offre_d_emploi\\\\chromedriver-win32\\\\chromedriver.exe\"\n",
    "options.binary_location = \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"  # Remplacez par l'emplacement réel de votre Chrome binary\n",
    "options.add_argument(f\"webdriver.chrome.driver={chrome_driver_path}\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Liste pour stocker les détails de chaque emploi\n",
    "all_job_details = []\n",
    "\n",
    "# Parcourir les liens\n",
    "for url in urls:\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    time.sleep(5)  # Attendre 5 secondes avant la prochaine requête\n",
    "\n",
    "    offres = soup.find_all('div', class_='box row')\n",
    "\n",
    "    # Parcourir les offres d'emploi sur la page principale\n",
    "    for offre in offres:\n",
    "        # Trouver la balise <h4> dans la structure HTML pour extraire le lien\n",
    "        offre_link_tag = offre.find('h4')\n",
    "\n",
    "        # Vérifier si la balise <h4> a été trouvée\n",
    "        if offre_link_tag:\n",
    "            # Extraire le lien de l'attribut 'href'\n",
    "            offre_link = offre_link_tag.find('a')['href']\n",
    "            all_job_details.append({'Offre_Link': offre_link})\n",
    "\n",
    "# Fermer le pilote Selenium à la fin\n",
    "driver.quit()\n",
    "\n",
    "# Concaténer tous les détails des emplois en un seul DataFrame\n",
    "if all_job_details:\n",
    "    all_job_details_df = pd.DataFrame(all_job_details)\n",
    "    # Afficher le DataFrame\n",
    "    #print(all_job_details_df)\n",
    "else:\n",
    "    print(\"Aucun détail d'offre d'emploi trouvé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce4a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_job_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "# Désactiver les avertissements liés aux requêtes non sécurisées\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# (votre fonction extract_job_information reste inchangée)\n",
    "def extract_job_information(soup):\n",
    "    # Extraction des informations de l'offre d'emploi\n",
    "    poste = soup.select_one('li.list-group-item:-soup-contains(\"Poste\")').strong.next_sibling.strip()\n",
    "    type_offre = soup.select_one('li.list-group-item:-soup-contains(\"Type d\\'offre\")').strong.next_sibling.strip()\n",
    "    metiers = soup.select_one('li.list-group-item:-soup-contains(\"Métier(s):\")').strong.next_sibling.strip()\n",
    "    niveaux = soup.select_one('li.list-group-item:-soup-contains(\"Niveau(x):\")').strong.next_sibling.strip()\n",
    "    experience = soup.select_one('li.list-group-item:-soup-contains(\"Expérience:\")').strong.next_sibling.strip()\n",
    "    lieu = soup.select_one('li.list-group-item:-soup-contains(\"Lieu:\")').strong.next_sibling.strip()\n",
    "    \n",
    "    # Extraction des dates de publication et de limite\n",
    "    date_publication = soup.find('strong', string='Date de publication:').find_next('span').text.strip()\n",
    "    date_limite = soup.find('strong', string='Date limite:').find_next('span').text.strip()\n",
    "    \n",
    "    # description = soup.select_one('div.text-col.post.small-post.col-md-9.col-xs-12 ul.list-group').text.strip()\n",
    "    description = soup.select_one('div.entry-content').text.strip()\n",
    "\n",
    "    return {\n",
    "        \"Poste\": [poste],\n",
    "        \"Type d'offre\": [type_offre],\n",
    "        \"Métier(s)\": [metiers],\n",
    "        \"Niveau(x)\": [niveaux],\n",
    "        \"Expérience\": [experience],\n",
    "        \"Lieu\": [lieu],\n",
    "        \"Date de publication\": [date_publication],\n",
    "        \"Date limite\": [date_limite],\n",
    "        \"Description\": [description]\n",
    "    }\n",
    "\n",
    "# Liste des liens\n",
    "urls = [\"https://emploi.educarriere.ci/nos-offres?page1={}&codes=&mots_cles=&typeemploi1=&niveau1=&anciennete=&typeoffre1=&recruteur=\".format(category) for category in range(40)]\n",
    "\n",
    "# En-tête pour éviter d'être bloqué\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Pour exécuter le navigateur en arrière-plan\n",
    "options.add_argument(\"--disable-gpu\")  # Désactiver l'accélération GPU en mode headless\n",
    "chrome_driver_path = \"C:\\\\Users\\\\ngora\\\\OneDrive\\\\Bureau\\\\INS_DATA\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "options.binary_location = \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"  # Remplacez par l'emplacement réel de votre Chrome binary\n",
    "options.add_argument(f\"webdriver.chrome.driver={chrome_driver_path}\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Liste pour stocker les DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Boucle sur chaque URL\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Envoyer une requête GET au site avec un délai de 120 secondes\n",
    "        req = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        time.sleep(5)  # Attendre 5 secondes avant la prochaine requête\n",
    "\n",
    "        offres = soup.find_all('div', class_='box row')\n",
    "\n",
    "        # Parcourir les offres d'emploi sur la page principale\n",
    "        for offre in offres:\n",
    "            offre_link_tag = offre.find('h4')\n",
    "            if offre_link_tag:\n",
    "                offre_link = offre_link_tag.find('a')['href']\n",
    "\n",
    "                # Ajouter l'URL à la liste des informations\n",
    "                all_job_details = {'Offre_Link': offre_link, 'URL': url}\n",
    "\n",
    "                try:\n",
    "                    # Extraire les informations sur l'emploi\n",
    "                    job_info = extract_job_information(soup)\n",
    "\n",
    "                    # Ajouter l'URL à la liste des informations\n",
    "                    job_info['URL'] = [url]\n",
    "\n",
    "                    # Créer un DataFrame\n",
    "                    df = pd.DataFrame(job_info)\n",
    "\n",
    "                    # Ajouter le DataFrame à la liste\n",
    "                    dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while extracting job information for URL {url}: {e}\")\n",
    "                    # En cas d'erreur, créer un DataFrame avec l'URL et une colonne vide\n",
    "                    df = pd.DataFrame({\"URL\": [url]})\n",
    "                    dfs.append(df)\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout lors de la requête pour l'URL {url}\")\n",
    "        # En cas de timeout, créer un DataFrame avec l'URL et une colonne vide\n",
    "        df = pd.DataFrame({\"URL\": [url]})\n",
    "        dfs.append(df)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Une erreur s'est produite lors de la requête pour l'URL {url}: {e}\")\n",
    "        # En cas d'autre exception, créer un DataFrame avec l'URL et une colonne vide\n",
    "        df = pd.DataFrame({\"URL\": [url]})\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Ajouter un délai de 5 secondes entre les requêtes pour éviter d'être bloqué\n",
    "    time.sleep(5)\n",
    "\n",
    "# Fermer le pilote Selenium à la fin\n",
    "driver.quit()\n",
    "\n",
    "# Concaténer tous les DataFrames en un seul DataFrame\n",
    "df_Educarriere = pd.concat(dfs, ignore_index=True)\n",
    "df_Educarriere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcb526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb1c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59428af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import time\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# (votre fonction extract_job_information reste inchangée)\n",
    "def extract_job_information(soup):\n",
    "    # Extraction des informations de l'offre d'emploi\n",
    "    poste = soup.select_one('li.list-group-item:-soup-contains(\"Poste\")').strong.next_sibling.strip()\n",
    "    type_offre = soup.select_one('li.list-group-item:-soup-contains(\"Type d\\'offre\")').strong.next_sibling.strip()\n",
    "    metiers = soup.select_one('li.list-group-item:-soup-contains(\"Métier(s):\")').strong.next_sibling.strip()\n",
    "    niveaux = soup.select_one('li.list-group-item:-soup-contains(\"Niveau(x):\")').strong.next_sibling.strip()\n",
    "    experience = soup.select_one('li.list-group-item:-soup-contains(\"Expérience:\")').strong.next_sibling.strip()\n",
    "    lieu = soup.select_one('li.list-group-item:-soup-contains(\"Lieu:\")').strong.next_sibling.strip()\n",
    "    \n",
    "    # Extraction des dates de publication et de limite\n",
    "    date_publication = soup.find('strong', string='Date de publication:').find_next('span').text.strip()\n",
    "    date_limite = soup.find('strong', string='Date limite:').find_next('span').text.strip()\n",
    "    \n",
    "   # description = soup.select_one('div.text-col.post.small-post.col-md-9.col-xs-12 ul.list-group').text.strip()\n",
    "    description = soup.select_one('div.entry-content').text.strip()\n",
    "\n",
    "    return {\n",
    "        \"Poste\": [poste],\n",
    "        \"Type d'offre\": [type_offre],\n",
    "        \"Métier(s)\": [metiers],\n",
    "        \"Niveau(x)\": [niveaux],\n",
    "        \"Expérience\": [experience],\n",
    "        \"Lieu\": [lieu],\n",
    "        \"Date de publication\": [date_publication],\n",
    "        \"Date limite\": [date_limite],\n",
    "        \"Description\": [description]\n",
    "    }\n",
    "\n",
    "# Liste des URLs à scraper\n",
    "urls = list(all_job_details_df['Offre_Link'])\n",
    "\n",
    "# Liste pour stocker les DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Boucle sur chaque URL\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Envoyer une requête GET au site avec un délai de 120 secondes\n",
    "        response = requests.get(url, headers=headers, verify=True, timeout=120)\n",
    "\n",
    "        # Vérifier si la requête a réussi (statut 200)\n",
    "        if response.status_code == 200:\n",
    "            # Analyser le contenu de la page avec BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                # Extraire les informations sur l'emploi\n",
    "                job_info = extract_job_information(soup)\n",
    "\n",
    "                # Créer un DataFrame\n",
    "                df = pd.DataFrame(job_info)\n",
    "\n",
    "                # Ajouter le DataFrame à la liste\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while extracting job information: {e}\")\n",
    "        else:\n",
    "            print(f\"Échec de la requête pour l'URL {url}. Statut : {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout lors de la requête pour l'URL {url}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Une erreur s'est produite lors de la requête pour l'URL {url}: {e}\")\n",
    "\n",
    "    # Ajouter un délai de 5 secondes entre les requêtes pour éviter d'être bloqué\n",
    "    time.sleep(5)\n",
    "\n",
    "# Concaténer tous les DataFrames en un seul DataFrame\n",
    "df_Educarriere = pd.concat(dfs, ignore_index=True)\n",
    "df_Educarriere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les listes existantes en tant que colonnes au DataFrame\n",
    "df_Educarriere['Poste'] = list(result_df['Poste'])\n",
    "df_Educarriere['Sous_titre'] = list(result_df['Sous_titre'])\n",
    "df_Educarriere['Code'] = list(result_df['Code'])\n",
    "df_Educarriere['Date_DEdition'] = list(result_df['Date_DEdition'])\n",
    "df_Educarriere['Date_limite'] = list(result_df['Date_limite'])\n",
    "df_Educarriere['Pays'] = list(result_df['Pays'])\n",
    "\n",
    "\n",
    "\n",
    "# Réorganiser les colonnes selon vos besoins\n",
    "df_Educarriere "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Exportez le DataFrame en fichier CSV\n",
    "df_Educarriere.to_csv(\"df_Educarriere.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6fb347",
   "metadata": {},
   "source": [
    "# NOVOJOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c8aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# Liste des liens pour chaque catégorie\n",
    "categories = [\n",
    "    \"toutes les offres d'emploi\",\n",
    "    \"juridique,fiscal,audit,conseil\",\n",
    "    \"administrations,moyens généraux\",\n",
    "    \"assistanat,secrétariat\",\n",
    "    \"metiers banque et assurances\",\n",
    "    \"RH,personnel,formation\",\n",
    "    \"education,enseignement\",\n",
    "    \"direction générale,direction d'unité\",\n",
    "    \"vente,televente,assistanat\",\n",
    "    \"commercial,technico commercial,service client\",\n",
    "    \"responsable commercial,grands comptes\",\n",
    "    \"créatio, design\",\n",
    "    \"marketing, communication\",\n",
    "    \"journalisme,médias,traduction\",\n",
    "    \"informatique,systèmes d'information,internet\",\n",
    "    \"télécommunication,réseaux\",\n",
    "    \"chantier,métiers BTP,architecture\",\n",
    "    \"ingénierie,etudes,projet,R&D\",\n",
    "    \"logistique,achat,stock,transport\",\n",
    "    \"production,méthode,industrie\",\n",
    "    \"maintenance,entretien\",\n",
    "    \"Qualité,sécurité,Environnement\",\n",
    "    \"Santé,Médical,Pharmacie\",\n",
    "    \"Hotellerie,Tourisme,Restauration, Loisirs\",\n",
    "    \"Ouvriers qualifiés, Chauffeur\",\n",
    "    \"autre\",\n",
    "    \"Métiers de l'agriculture\"\n",
    "]\n",
    "\n",
    "base_url = \"https://www.novojob.com/cote-d-ivoire/offres-d-emploi?q=\"\n",
    "category_links = [f\"{base_url}{'+'.join(category.split(','))}\" for category in categories]\n",
    "\n",
    "intitules_list = []\n",
    "entreprises_list = []\n",
    "pays_list = []\n",
    "dates_list = []\n",
    "lien_list = []\n",
    "niveau_list = []\n",
    "experience_list = []\n",
    "\n",
    "# Utilisation d'un en-tête pour éviter d'être bloqué\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Parcourir les liens de chaque catégorie\n",
    "for category_link in category_links:\n",
    "    req = requests.get(category_link, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    time.sleep(5)  # Attendre 5 secondes avant la prochaine requête\n",
    "\n",
    "    if 'finance' in category_link:\n",
    "        offres = soup.find_all('h2', class_='ellipsis row-fluid')\n",
    "        entreprises = soup.find_all('h6', class_='ellipsis')\n",
    "        niveaux = soup.find_all('span', class_='spaced-right phone-display-blok')\n",
    "    else:\n",
    "        offres = soup.find_all('h2', class_='ellipsis row-fluid')\n",
    "        entreprises = soup.find_all('h6', class_='ellipsis')\n",
    "        niveaux = soup.find_all('span', class_='spaced-right phone-display-blok')\n",
    "\n",
    "    for offre, entreprise, niveau in zip(offres, entreprises, niveaux):\n",
    "        bloc_bottom = offre.find_next('div', class_='bloc-bottom')\n",
    "        intitules_list.append(offre.get_text().strip())\n",
    "        entreprises_list.append(entreprise.get_text().strip())\n",
    "        lien_list.append(category_links.index(category_link))\n",
    "\n",
    "        # Les informations (pays, date, niveau, expérience) sont contenues dans la même span, nous devons les séparer\n",
    "        pays_info = bloc_bottom.find('i', class_='fa fa-map-marker icon-left')\n",
    "        pays = pays_info.find_parent().text.strip() if pays_info else None\n",
    "        pays_list.append(pays)\n",
    "\n",
    "        date_info = bloc_bottom.find('i', class_='fa fa-clock-o icon-left')\n",
    "        date = date_info.find_parent().text.strip() if date_info else None\n",
    "        dates_list.append(date)\n",
    "\n",
    "        # Ajout des colonnes pour le niveau du poste et l'expérience demandée\n",
    "        niveau_info = niveau.find('i', class_='fa fa-bookmark icon-left')\n",
    "        niveau_text = niveau_info.find_parent().text.strip() if niveau_info else None\n",
    "\n",
    "        # Utiliser une expression régulière pour extraire les informations de niveau et d'expérience\n",
    "        match = re.match(r'(.+) \\((.+)\\)', niveau_text)\n",
    "\n",
    "        if match:\n",
    "            niveau_col, experience_col = match.groups()\n",
    "        else:\n",
    "            niveau_col, experience_col = None, None\n",
    "\n",
    "        niveau_list.append(niveau_col)\n",
    "        experience_list.append(experience_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35558398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers = pd.DataFrame({\n",
    "    'Intitule': intitules_list,\n",
    "    'Entreprise': entreprises_list,\n",
    "    'Pays': pays_list,\n",
    "    'Date': dates_list,\n",
    "    'Niveau': niveau_list,\n",
    "    'Experience_lettre': experience_list,\n",
    "    'Lien': lien_list\n",
    "})\n",
    "df_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc16d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# Liste des liens pour chaque catégorie\n",
    "categories = [\n",
    "    \"toutes les offres d'emploi\",\n",
    "    \"juridique,fiscal,audit,conseil\",\n",
    "    \"administrations,moyens généraux\",\n",
    "    \"assistanat,secrétariat\",\n",
    "    \"metiers banque et assurances\",\n",
    "    \"RH,personnel,formation\",\n",
    "    \"education,enseignement\",\n",
    "    \"direction générale,direction d'unité\",\n",
    "    \"vente,televente,assistanat\",\n",
    "    \"commercial,technico commercial,service client\",\n",
    "    \"responsable commercial,grands comptes\",\n",
    "    \"créatio, design\",\n",
    "    \"marketing, communication\",\n",
    "    \"journalisme,médias,traduction\",\n",
    "    \"informatique,systèmes d'information,internet\",\n",
    "    \"télécommunication,réseaux\",\n",
    "    \"chantier,métiers BTP,architecture\",\n",
    "    \"ingénierie,etudes,projet,R&D\",\n",
    "    \"logistique,achat,stock,transport\",\n",
    "    \"production,méthode,industrie\",\n",
    "    \"maintenance,entretien\",\n",
    "    \"Qualité,sécurité,Environnement\",\n",
    "    \"Santé,Médical,Pharmacie\",\n",
    "    \"Hotellerie,Tourisme,Restauration, Loisirs\",\n",
    "    \"Ouvriers qualifiés, Chauffeur\",\n",
    "    \"autre\",\n",
    "    \"Métiers de l'agriculture\"\n",
    "]\n",
    "\n",
    "base_url = \"https://www.novojob.com/cote-d-ivoire/offres-d-emploi?q=\"\n",
    "category_links = [f\"{base_url}{'+'.join(category.split(','))}\" for category in categories]\n",
    "\n",
    "# Utilisation d'un en-tête pour éviter d'être bloqué\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Initialiser le pilote Selenium\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Liste pour stocker les détails de chaque emploi\n",
    "all_job_lien = []\n",
    "\n",
    "# Parcourir les liens de chaque catégorie\n",
    "for category_link in category_links:\n",
    "    req = requests.get(category_link, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    time.sleep(5)  # Attendre 5 secondes avant la prochaine requête\n",
    "\n",
    "    offres = soup.find_all('div', class_='row-fluid job-details pointer')\n",
    "\n",
    "    # Parcourir les offres d'emploi sur la page principale\n",
    "    for offre in offres:\n",
    "        # Trouver la balise <a> dans la structure HTML pour extraire le lien\n",
    "        offre_link_tag = offre.find('a')\n",
    "\n",
    "        # Vérifier si la balise <a> a été trouvée\n",
    "        if offre_link_tag:\n",
    "            # Extraire le lien de l'attribut 'href'\n",
    "            offre_link = offre_link_tag['href']\n",
    "\n",
    "            \n",
    "            # Ajouter les détails de l'emploi à la liste\n",
    "            all_job_lien.append(offre_link)\n",
    "\n",
    "# Fermer le pilote Selenium à la fin\n",
    "driver.quit()\n",
    "\n",
    "# Convertir les détails des offres d'emploi en DataFrame\n",
    "# Convertir les détails des offres d'emploi en DataFrame\n",
    "df_Novojob = pd.DataFrame(all_job_lien, columns=['Offre_Link'])\n",
    "df_Novojob.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Example usage for multiple job URLs\n",
    "job_urls = list(df_Novojob['Offre_Link'])\n",
    "\n",
    "# Extract details for each job URL\n",
    "all_job_details = []\n",
    "for url in job_urls:\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    job_details = {}\n",
    "    \n",
    "    # Ajouter le lien\n",
    "    job_details[\"Offre_Link\"]=url\n",
    "    # Extracting job details\n",
    "    details_section = soup.find('ul', class_='text-small')\n",
    "    if details_section:\n",
    "        for li in details_section.find_all('li', class_='row-fluid'):\n",
    "            key = li.find('span', class_='span4').text.strip()\n",
    "            value = li.find('span', class_='span8').text.strip()\n",
    "            job_details[key] = value\n",
    "\n",
    "    # Extracting the provided text\n",
    "    description_section = soup.find('div', class_='spaced details-description')\n",
    "    if description_section:\n",
    "        provided_text = description_section.text.strip()\n",
    "        job_details['Provided Text'] = provided_text\n",
    "\n",
    "    all_job_details.append(job_details)\n",
    "\n",
    "# Create a DataFrame\n",
    "df_offers = pd.DataFrame(all_job_details)\n",
    "df_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Example usage for multiple job URLs\n",
    "job_urls = list(df_Novojob['Offre_Link'])\n",
    "\n",
    "# Extract details for each job URL\n",
    "all_job_details = []\n",
    "for url in job_urls:\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    job_details = {}\n",
    "\n",
    "    # Extracting job details\n",
    "    details_section = soup.find('ul', class_='text-small')\n",
    "    if details_section:\n",
    "        for li in details_section.find_all('li', class_='row-fluid'):\n",
    "            key = li.find('span', class_='span4').text.strip()\n",
    "            value = li.find('span', class_='span8').text.strip()\n",
    "            job_details[key] = value\n",
    "\n",
    "    # Extracting the provided text\n",
    "    description_section = soup.find('div', class_='spaced details-description')\n",
    "    if description_section:\n",
    "        provided_text = description_section.text.strip()\n",
    "        job_details['Provided Text'] = provided_text\n",
    "\n",
    "    all_job_details.append(job_details)\n",
    "\n",
    "# Create a DataFrame\n",
    "df_offers = pd.DataFrame(all_job_details)\n",
    "df_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les listes existantes en tant que colonnes au DataFrame\n",
    "df_offers['Intitule'] = intitules_list\n",
    "df_offers['Entreprise'] = entreprises_list\n",
    "df_offers['Pays'] = pays_list\n",
    "df_offers['Date'] = dates_list\n",
    "df_offers['Niveau'] = niveau_list\n",
    "df_offers['Experience_lettre'] = experience_list\n",
    "df_offers['Lien'] = lien_list\n",
    "\n",
    "# Réorganiser les colonnes selon vos besoins\n",
    "\n",
    "df_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ada053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Réorganiser les colonnes selon vos besoins\n",
    "df_offers = df_offers[[\n",
    "    'Intitule', 'Entreprise', 'Pays', 'Date', 'Niveau', 'Experience_lettre',\n",
    "    'Lien', \"Lieu de travail\", \"Date d'expiration\", 'Niveau de poste', \"Secteur d'activité\", \"Niveau d'étude (diplome)\",\n",
    "    \"Nombre de postes\", \"Type de contrat\", \"Provided Text\", \"Nom de l'entreprise\"\n",
    "]]\n",
    "\n",
    "\n",
    "df_offers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09153451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportez le DataFrame en fichier CSV\n",
    "df_offers.to_csv(\"df_Novojob.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535849ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Charger les données\n",
    "# Assurez-vous d'avoir préalablement importé les bibliothèques nécessaires et défini df_offers\n",
    "# df_offers = pd.read_csv(\"votre_fichier.csv\")  # Remplacez \"votre_fichier.csv\" par votre fichier de données\n",
    "\n",
    "# Réorganiser les colonnes selon vos besoins\n",
    "df_offers = df_offers[[\n",
    "    'Intitule', 'Entreprise', 'Pays', 'Date', 'Niveau', 'Experience_lettre',\n",
    "    'Lien', \"Lieu de travail\", \"Date d'expiration\", 'Niveau de poste', \"Secteur d'activité\", \"Niveau d'étude (diplome)\",\n",
    "    \"Nombre de postes\", \"Type de contrat\", \"Provided Text\", \"Nom de l'entreprise\"\n",
    "]]\n",
    "\n",
    "# Options pour les filtres\n",
    "secteurs_activite_options = [{'label': 'Tout sélectionner', 'value': 'all'}] + [{'label': secteur, 'value': secteur} for secteur in df_offers[\"Secteur d'activité\"].unique()]\n",
    "types_contrat_options = [{'label': 'Tout sélectionner', 'value': 'all'}] + [{'label': contrat, 'value': contrat} for contrat in df_offers[\"Type de contrat\"].unique() if pd.notna(contrat)]\n",
    "\n",
    "# Palettes de couleurs\n",
    "secteur_activite_palette = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "entreprise_palette = ['#ffbb78', '#98df8a', '#ff9896', '#c5b0d5', '#c49c94', '#f7b6d2', '#dbdb8d', '#9edae5', '#ff7f0e', '#aec7e8']\n",
    "type_contrat_palette = ['#ff7f0e', '#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "# Initialiser l'application Dash\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Définir la mise en page du tableau de bord\n",
    "app.layout = html.Div(children=[\n",
    "    html.H1(children='Tableau de Bord des Offres d\\'Emploi'),\n",
    "\n",
    "    # Filtres interactifs\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.Label('Filtrer par Secteur d\\'activité:'),\n",
    "            dcc.Dropdown(\n",
    "                id='filtre-secteur-activite',\n",
    "                options=secteurs_activite_options,\n",
    "                multi=True,\n",
    "                placeholder='Sélectionner un secteur d\\'activité'\n",
    "            ),\n",
    "        ], style={'flex': '1'}),\n",
    "\n",
    "        html.Div([\n",
    "            html.Label('Filtrer par Type de contrat:'),\n",
    "            dcc.Dropdown(\n",
    "                id='filtre-type-contrat',\n",
    "                options=types_contrat_options,\n",
    "                multi=True,\n",
    "                placeholder='Sélectionner un type de contrat'\n",
    "            ),\n",
    "        ], style={'flex': '1'}),\n",
    "\n",
    "        html.Button('Réinitialiser les filtres', id='bouton-reset', n_clicks=0)\n",
    "    ], style={'display': 'flex'}),\n",
    "\n",
    "    # Graphique du nombre d'offres par secteur d'activité\n",
    "    dcc.Graph(\n",
    "        id='graph-secteur-activite',\n",
    "        style={'height': '50vh'},\n",
    "    ),\n",
    "\n",
    "    # Graphique du nombre d'offres par entreprise\n",
    "    dcc.Graph(\n",
    "        id='graph-entreprise',\n",
    "        style={'height': '50vh'},\n",
    "    ),\n",
    "\n",
    "    # Graphique du type de contrat proposé\n",
    "    dcc.Graph(\n",
    "        id='graph-type-contrat',\n",
    "        style={'height': '50vh'},\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Callback pour mettre à jour les graphiques en fonction des filtres\n",
    "@app.callback(\n",
    "    [Output('graph-secteur-activite', 'figure'),\n",
    "     Output('graph-entreprise', 'figure'),\n",
    "     Output('graph-type-contrat', 'figure')],\n",
    "    [Input('filtre-secteur-activite', 'value'),\n",
    "     Input('filtre-type-contrat', 'value'),\n",
    "     Input('bouton-reset', 'n_clicks')]\n",
    ")\n",
    "def update_graphs(secteurs_activite, types_contrat, n_clicks_reset):\n",
    "    # Filtrage du DataFrame en fonction des sélections\n",
    "    df_filtered = df_offers\n",
    "\n",
    "    # Filtrer par secteur d'activité\n",
    "    if secteurs_activite and 'all' not in secteurs_activite:\n",
    "        df_filtered = df_filtered[df_filtered[\"Secteur d'activité\"].isin(secteurs_activite)]\n",
    "\n",
    "    # Filtrer par type de contrat\n",
    "    if types_contrat and 'all' not in types_contrat:\n",
    "        df_filtered = df_filtered[df_filtered[\"Type de contrat\"].isin(types_contrat)]\n",
    "\n",
    "    # Graphique du nombre d'offres par secteur d'activité\n",
    "    fig_secteur_activite = px.bar(df_filtered, x=\"Secteur d'activité\", title=\"Nombre d'offres par secteur d'activité\",\n",
    "                                   color_discrete_sequence=secteur_activite_palette)\n",
    "\n",
    "    # Graphique du nombre d'offres par entreprise\n",
    "    fig_entreprise = px.bar(df_filtered, x=\"Nom de l'entreprise\", title=\"Nombre d'offres par entreprise\",\n",
    "                            color_discrete_sequence=entreprise_palette)\n",
    "\n",
    "    # Graphique du type de contrat proposé\n",
    "    fig_type_contrat = px.bar(df_filtered, x=\"Type de contrat\", title=\"Type de contrat proposé\",\n",
    "                              color_discrete_sequence=type_contrat_palette)\n",
    "\n",
    "    return fig_secteur_activite, fig_entreprise, fig_type_contrat\n",
    "\n",
    "# Exécuter l'application Dash\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f90dc4",
   "metadata": {},
   "source": [
    "# 1. Analyse descriptive :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c82d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Afficher les statistiques descriptives\n",
    "descriptive_stats = df_offers.describe(include='all')\n",
    "print(descriptive_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba64489",
   "metadata": {},
   "source": [
    "# 2. Analyse comparative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad38c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison entre différents secteurs\n",
    "secteur_comparison = df_offers.groupby(\"Secteur d'activité\").size()\n",
    "print(secteur_comparison)\n",
    "\n",
    "# Comparaison entre différentes régions\n",
    "region_comparison = df_offers.groupby(\"Pays\").size()\n",
    "print(region_comparison)\n",
    "\n",
    "# Comparaison entre différents niveaux d'expérience\n",
    "experience_comparison = df_offers.groupby(\"Niveau\").size()\n",
    "print(experience_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7aa9a",
   "metadata": {},
   "source": [
    "# 3. Modélisation :\n",
    "Si vous souhaitez prédire des tendances futures, vous pourriez envisager une modélisation statistique, par exemple une régression pour estimer les relations entre différentes variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ee05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Exemple de modélisation simple (n'utilisez que les variables numériques)\n",
    "X = df_offers[['Nombre de postes']]\n",
    "y = df_offers['Niveau de poste']\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création et ajustement du modèle de régression linéaire\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7922e8",
   "metadata": {},
   "source": [
    "# 4. Interprétation des résultats :\n",
    "Analysez les résultats de manière critique, identifiez les tendances, les corrélations, et les relations significatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde7eb46",
   "metadata": {},
   "source": [
    "# 5. Rapport :\n",
    "Présentez vos résultats de manière claire dans un rapport en utilisant des graphiques et des tableaux pour illustrer vos conclusions. Vous pouvez utiliser des bibliothèques comme matplotlib ou seaborn pour créer des visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991432a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Exemple de graphique : Nombre d'offres par secteur d'activité\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x=\"Secteur d'activité\", data=df_offers)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Nombre d\\'offres par secteur d\\'activité')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b014fe25",
   "metadata": {},
   "source": [
    "<p align=\"center\" style=\"color: #4CAF50; font-size: 1.2em;\">\n",
    "  NLP\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c557414a",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color: #FFA500; font-size: 2.5em; font-weight: bold;\">1. Analyse du Texte de l'Offre :</h1>\n",
    "<p align=\"center\" style=\"color: #4CAF50; font-size: 1.2em;\">\n",
    " Vous pouvez utiliser NLP pour extraire des informations clés des descriptions d'offres d'emploi. Par exemple, pour extraire les compétences requises, les responsabilités, et d'autres informations pertinentes :\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5093d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Charger le modèle NLP\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Appliquer NLP sur le texte de l'offre\n",
    "texte_offre = df_offers['Provided Text'].iloc[0]  # Utilisez l'indice approprié\n",
    "doc = nlp(texte_offre)\n",
    "\n",
    "# Extraire les entités nommées\n",
    "entites = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(\"Entités nommées :\", entites)\n",
    "\n",
    "# Extraire les compétences\n",
    "competences = [token.text for token in doc if \"compétence\" in token.text.lower()]\n",
    "print(\"Compétences requises :\", competences)\n",
    "\n",
    "# D'autres analyses NLP peuvent également être appliquées selon les besoins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e78a4c",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color: #FFA500; font-size: 2.5em; font-weight: bold;\">2. Analyse du Sentiment :</h1>\n",
    "<p align=\"center\" style=\"color: #4CAF50; font-size: 1.2em;\">\n",
    " Vous pouvez utiliser NLP pour évaluer le sentiment général du texte de l'offre, ce qui peut donner une idée de l'atmosphère de travail décrite :\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Appliquer TextBlob sur le texte de l'offre\n",
    "analyse_sentiment = TextBlob(texte_offre)\n",
    "\n",
    "# Extraire le sentiment (positif, négatif, neutre)\n",
    "sentiment = analyse_sentiment.sentiment.polarity\n",
    "print(\"Sentiment de l'offre :\", sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483bc7b4",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color: #FFA500; font-size: 2.5em; font-weight: bold;\">3. Modélisation de Thèmes :</h1>\n",
    "<p align=\"center\" style=\"color: #4CAF50; font-size: 1.2em;\">\n",
    " L'utilisation de techniques NLP comme le Latent Dirichlet Allocation (LDA) peut vous aider à identifier les thèmes clés présents dans les offres d'emploi :\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dcc6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Utiliser CountVectorizer pour convertir le texte en vecteurs\n",
    "vectorizer = CountVectorizer(stop_words='french')\n",
    "X = vectorizer.fit_transform(df_offers['Provided Text'])\n",
    "\n",
    "# Appliquer le modèle LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Afficher les mots clés de chaque thème\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    mots_cles = [vectorizer.get_feature_names_out()[j] for j in topic.argsort()[-10:]]\n",
    "    print(f\"Thème {i+1} :\", mots_cles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342bf08a",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color: #FFA500; font-size: 2.5em; font-weight: bold;\">4. Identification des Relations :</h1>\n",
    "<p align=\"center\" style=\"color: #4CAF50; font-size: 1.2em;\">\n",
    " Vous pouvez utiliser NLP pour identifier des relations entre différentes entités dans le texte, par exemple, entre le lieu de travail et le secteur d'activité.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'identification de relations entre le lieu de travail et le secteur d'activité\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"LOC\":\n",
    "        lieu_travail = ent.text\n",
    "    elif ent.label_ == \"ORG\":\n",
    "        secteur_activite = ent.text\n",
    "\n",
    "print(f\"Lieu de travail : {lieu_travail}, Secteur d'activité : {secteur_activite}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34b56a",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\" style=\"color: #4CAF50; font-size: 1.2em;\">\n",
    " d'application de NLP sur la colonne \"Provided Text\" pour extraire des entités nommées, des compétences et effectuer une analyse de sentiment.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1302d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e1996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Charger le modèle NLP\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Appliquer NLP sur le texte de l'offre (par exemple, prenons la première offre)\n",
    "texte_offre = df_offers['Provided Text'].iloc[0]\n",
    "doc = nlp(texte_offre)\n",
    "\n",
    "# Extraire les entités nommées\n",
    "entites = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(\"Entités nommées :\", entites)\n",
    "\n",
    "# Extraire les compétences\n",
    "competences = [token.text for token in doc if \"compétence\" in token.text.lower()]\n",
    "print(\"Compétences requises :\", competences)\n",
    "\n",
    "# Appliquer TextBlob pour analyser le sentiment\n",
    "analyse_sentiment = TextBlob(texte_offre)\n",
    "sentiment = analyse_sentiment.sentiment.polarity\n",
    "print(\"Sentiment de l'offre :\", sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa5ab6",
   "metadata": {},
   "source": [
    "# Emploi.ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d61f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text(element, tag_name=None):\n",
    "    tag = element.find(tag_name)\n",
    "    return tag.text.strip() if tag else \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace('D\\x92', ' ').replace('d\\x92', ' ').replace('\\x92', ' ').replace('\\r\\n', '').replace('\\xa0', '')\n",
    "\n",
    "def scrape_emploi_ci(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=500)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur de connexion à {url} : {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    job_description_wrappers = soup.find_all('div', class_='job-description-wrapper')\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for wrapper in job_description_wrappers:\n",
    "        h5_tag = wrapper.find('h5')\n",
    "        poste = extract_text(h5_tag, 'a')\n",
    "\n",
    "        job_recruiter_tag = wrapper.find('p', class_='job-recruiter')\n",
    "        date_and_company = job_recruiter_tag.text.strip().split('|')\n",
    "        date = date_and_company[0].strip() if date_and_company else \"\"\n",
    "        entreprise = extract_text(job_recruiter_tag, 'a')\n",
    "\n",
    "        description_tag = wrapper.find('div', class_='search-description')\n",
    "        description = clean_text(description_tag.text.strip()) if description_tag else \"\"\n",
    "\n",
    "        region_tag = wrapper.find('p', text='Région de :')\n",
    "        region = extract_text(region_tag) if region_tag else \"\"\n",
    "\n",
    "        data_list.append({\n",
    "            'Poste': poste,\n",
    "            'Entreprise': entreprise,\n",
    "            'Date': date,\n",
    "            'Description': description,\n",
    "            'Région': region\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "# Liste des liens\n",
    "categories = [\"31\", \"1127\", \"29\", \"37\", \"1115\", \"30\", \"1115\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"39\", \"38\", \"40\", \"525\", \"41\", \"28\"]\n",
    "#categories=[\"31\"]\n",
    "# Liste d'URLs générées\n",
    "urls = [\"https://www.emploi.ci/recherche-jobs-cote-ivoire/?f%5B0%5D=im_field_offre_metiers%3A{}\".format(category) for category in categories]\n",
    "\n",
    "# Créer un DataFrame à partir des liens\n",
    "df = pd.concat([scrape_emploi_ci(url) for url in urls], ignore_index=True)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08969aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# Liste des liens\n",
    "urls = [\"https://www.emploi.ci/recherche-jobs-cote-ivoire/?f%5B0%5D=im_field_offre_metiers%3A{}\".format(category) for category in categories]\n",
    "\n",
    "\n",
    "# En-tête pour éviter d'être bloqué\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Pour exécuter le navigateur en arrière-plan\n",
    "options.add_argument(\"--disable-gpu\")  # Désactiver l'accélération GPU en mode headless\n",
    "chrome_driver_path = \"C:\\\\Users\\\\Dell\\\\Desktop\\\\offre_d_emploi\\\\chromedriver-win32\\\\chromedriver.exe\"\n",
    "options.binary_location = \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"  # Remplacez par l'emplacement réel de votre Chrome binary\n",
    "options.add_argument(f\"webdriver.chrome.driver={chrome_driver_path}\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Liste pour stocker les détails de chaque emploi\n",
    "all_job_details = []\n",
    "\n",
    "# Parcourir les liens\n",
    "for url in urls:\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    time.sleep(5)  # Attendre 5 secondes avant la prochaine requête\n",
    "\n",
    "    offres = soup.find_all('div', class_=\"job-description-wrapper\")\n",
    "\n",
    "    # Parcourir les offres d'emploi sur la page principale\n",
    "    for offre in offres:\n",
    "        # Trouver la balise <h4> dans la structure HTML pour extraire le lien\n",
    "        offre_link_tag = offre.find('h5')\n",
    "\n",
    "        # Vérifier si la balise <h4> a été trouvée\n",
    "        if offre_link_tag:\n",
    "            # Extraire le lien de l'attribut 'href'\n",
    "            offre_link = offre_link_tag.find('a')['href']\n",
    "            all_job_details.append({'Offre_Link': \"https://www.emploi.ci\"+offre_link})\n",
    "\n",
    "# Fermer le pilote Selenium à la fin\n",
    "driver.quit()\n",
    "\n",
    "# Concaténer tous les détails des emplois en un seul DataFrame\n",
    "if all_job_details:\n",
    "    all_job_details_df = pd.DataFrame(all_job_details)\n",
    "    # Afficher le DataFrame\n",
    "    #print(all_job_details_df)\n",
    "else:\n",
    "    print(\"Aucun détail d'offre d'emploi trouvé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import ChunkedEncodingError, ConnectionError, ReadTimeout\n",
    "\n",
    "# Fonction pour extraire les informations d'une page\n",
    "def extract_information(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=120)  # Augmentation du délai à 20 secondes\n",
    "        response.raise_for_status()\n",
    "        response.encoding = 'utf-8'\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extraction des informations sur l'entreprise\n",
    "        company_info = soup.select_one('.job-ad-company')\n",
    "        entreprise = {\n",
    "            'Nom': company_info.select_one('.company-title a').text.strip() if company_info and company_info.select_one('.company-title a') else None,\n",
    "            'Secteur d´activité': ', '.join(item.text.strip() for item in company_info.select('.sector-title .field-item')) if company_info and company_info.select('.sector-title .field-item') else None,\n",
    "            'Description de l\\'entreprise': soup.select_one('.job-ad-company-description label + *').text.strip() if soup.select_one('.job-ad-company-description label + *') else None\n",
    "        }\n",
    "\n",
    "        # Extraction des informations sur l'annonce\n",
    "        annonce_info = soup.select_one('.job-ad-details')\n",
    "        annonce = {\n",
    "            'Poste': soup.select_one('.ad-ss-title').text.strip() if soup.select_one('.ad-ss-title') else None,\n",
    "            'Missions': [li.text.strip() for li in soup.select('.content ul.missions li')] if soup.select('.content ul.missions') else None,\n",
    "            'Profil recherché': [li.text.strip() for li in soup.select('.content ul.profil li')] if soup.select('.content ul.profil') else None,\n",
    "            'Métier': soup.select_one('.job-ad-criteria .field-name-field-offre-metiers .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-metiers .field-item') else None,\n",
    "            'Secteur d´activité (de l\\'annonce)': soup.select_one('.job-ad-criteria .field-name-field-offre-secteur .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-secteur .field-item') else None,\n",
    "            'Type de contrat': soup.select_one('.job-ad-criteria .field-name-field-offre-contrat-type .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-contrat-type .field-item') else None,\n",
    "            'Région': soup.select_one('.job-ad-criteria .field-name-field-offre-region .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-region .field-item') else None,\n",
    "            'Ville': soup.select_one('.job-ad-criteria .field-name-field-offre-ville .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-ville .field-item') else None,\n",
    "            'Niveau d\\'expérience': soup.select_one('.job-ad-criteria .field-name-field-offre-niveau-experience .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-niveau-experience .field-item') else None,\n",
    "            'Niveau d\\'études': soup.select_one('.job-ad-criteria .field-name-field-offre-niveau-etude .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-niveau-etude .field-item') else None,\n",
    "            'Compétences clés': [li.text.strip() for li in soup.select('.job-ad-criteria .field-name-field-offre-tags .field-item')] if soup.select('.job-ad-criteria .field-name-field-offre-tags .field-item') else None,\n",
    "            'Nombre de poste(s)': soup.select_one('.job-ad-criteria td:contains(\"Nombre de poste(s) :\") + td').text.strip() if soup.select_one('.job-ad-criteria td:contains(\"Nombre de poste(s) :\") + td') else None,\n",
    "}\n",
    "\n",
    "        return {'entreprise': entreprise, 'annonce': annonce}\n",
    "\n",
    "    except (ConnectionError, ReadTimeout, ChunkedEncodingError) as e:\n",
    "        print(f\"Erreur lors de la requête {url}: {e}\")\n",
    "        # Relancer la requête\n",
    "        return None\n",
    "\n",
    "# Liste des URLs\n",
    "urls = list(all_job_details_df['Offre_Link'])\n",
    "\n",
    "# Initialisation d'une liste pour stocker les DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Boucle à travers chaque URL\n",
    "for url in urls:\n",
    "    data = extract_information(url)\n",
    "\n",
    "    # Si la requête a échoué, passez à l'URL suivante\n",
    "    if data is None:\n",
    "        continue\n",
    "\n",
    "    # Création du DataFrame pour chaque URL\n",
    "    df = pd.DataFrame([data['entreprise'] | data['annonce']])\n",
    "\n",
    "    # Ajout du DataFrame à la liste\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concaténation des DataFrames de chaque URL\n",
    "result_df = pd.concat(df_list, ignore_index=True)\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les listes existantes en tant que colonnes au DataFrame\n",
    "result_df['Poste'] = list(df['Poste'])\n",
    "result_df['Entreprise'] = list(df['Entreprise'])\n",
    "result_df['Date'] =  list(df['Date'])\n",
    "result_df['Description'] =  list(df['Description'])\n",
    "result_df['Région'] =  list(df['Région'])\n",
    "\n",
    "# Réorganiser les colonnes selon vos besoins\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ab57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportez le DataFrame en fichier CSV\n",
    "result_df.to_csv(\"df_Novojob.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e655b5",
   "metadata": {},
   "source": [
    "# ci.talent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text(element, tag_name=None):\n",
    "    tag = element.find(tag_name)\n",
    "    return tag.text.strip() if tag else \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "\n",
    "def scrape_talent_com(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=500)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur de connexion à {url} : {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    job_wrappers = soup.find_all('div', class_='card card__job')\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for wrapper in job_wrappers:\n",
    "        title_tag = wrapper.find('h2', class_='card__job-title')\n",
    "        title = extract_text(title_tag, 'a')\n",
    "        \n",
    "        employer_location_tag = wrapper.find('div', class_='card__job-empnameLocation')\n",
    "        #employer = extract_text(employer_location_tag.find('div', class_='card__job-location'))  # Extract location from the inner div\n",
    "        location= extract_text(employer_location_tag, 'div')\n",
    "\n",
    "        employer_location_tag = wrapper.find('div', class_='card__job-empname-label')\n",
    "        \n",
    "        # Extracting employer and description from the div\n",
    "        employer = employer_location_tag.text.strip() if employer_location_tag else None  # Extract location from the inner div\n",
    "        \n",
    "        description_tag = wrapper.find('div', class_='card__job-snippet-logo')\n",
    "        description = clean_text(extract_text(description_tag, 'p'))\n",
    "\n",
    "        data_list.append({\n",
    "            'Title': title,\n",
    "            'Location': location,\n",
    "            'Employer': employer,\n",
    "            'Description': description\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "# List of URLs for talent.com jobs\n",
    "urls = [\n",
    "     \"https://ci.talent.com/jobs?l=Abidjan%2C+Abidjan&radius=15&p={}&k=&context=serp_pagination\".format(category) for category in range(8)\n",
    "    # Add more URLs as needed \"https://ci.talent.com/jobs\",\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Scrape job information for each URL and concatenate the results\n",
    "for url in urls:\n",
    "    df = scrape_talent_com(url)\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8308bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportez le DataFrame en fichier CSV\n",
    "combined_df.to_csv(\"d_talent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc1daa",
   "metadata": {},
   "source": [
    "# projobivoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db79e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text(element, tag_name=None):\n",
    "    if element and tag_name:\n",
    "        tag = element.find(tag_name)\n",
    "        return tag.text.strip() if tag else \"\"\n",
    "    return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "\n",
    "def scrape_projobivoire_page(page_url):\n",
    "    job_data_list = []\n",
    "\n",
    "    for url in page_url:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=500)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            \n",
    "            print(f\"Erreur de connexion à {url} : {e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        job_items = soup.find_all('div', class_='loop-item-wrap list')\n",
    "\n",
    "        if not job_items:\n",
    "            \n",
    "            print(f\"Aucun élément de travail trouvé pour l'URL : {url}\")\n",
    "            continue\n",
    "\n",
    "        for job_item in job_items:\n",
    "            title_tag = job_item.find('h3', class_='loop-item-title')\n",
    "            title = extract_text(title_tag, 'a')\n",
    "\n",
    "            job_type_tag = job_item.find('span', class_='job-type')\n",
    "            job_type = extract_text(job_type_tag, 'span')\n",
    "\n",
    "            job_date_posted = soup.find('span', class_='job-date__posted').text.strip()\n",
    "\n",
    "            job_date_closing_tag = soup.find('span', class_='job-date__closing')\n",
    "            job_date_closing = job_date_closing_tag.text.strip() if job_date_closing_tag else \"\"\n",
    "\n",
    "            job_date_closing = job_date_closing.lstrip('-').strip()\n",
    "\n",
    "            category_tag = job_item.find('span', class_='job-category')\n",
    "            category = extract_text(category_tag, 'a')\n",
    "\n",
    "            # Ajout de ces lignes pour extraire l'URL de l'e-mail\n",
    "            email_url_tag = job_item.find('span', class_='noo-tool-email-job')\n",
    "            email_url = email_url_tag['data-url'] if email_url_tag else \"\"\n",
    "\n",
    "            data = {\n",
    "                'Title': title,\n",
    "                'Type': job_type,\n",
    "                'DatePosted': job_date_posted,\n",
    "                'DateClosing': job_date_closing,\n",
    "                'Category': category,\n",
    "                'EmailURL': email_url,\n",
    "            }\n",
    "\n",
    "            job_data_list.append(data)\n",
    "\n",
    "    return job_data_list\n",
    "\n",
    "# Liste des URL de pages avec plusieurs offres d'emploi\n",
    "page_urls = [\"https://projobivoire.com/page/{}/\".format(category) for category in range(546)]\n",
    "\n",
    "# Scrape des détails de chaque offre d'emploi sur les pages\n",
    "job_data_list = scrape_projobivoire_page(page_urls)\n",
    "\n",
    "# Création d'un DataFrame à partir de la liste des données d'emploi\n",
    "df_projobivoire = pd.DataFrame(job_data_list)\n",
    "\n",
    "# Affichage du DataFrame\n",
    "df_projobivoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82246ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projobivoire[\"EmailURL\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70226731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_job_information(soup):\n",
    "    # Extraction des informations de l'offre d'emploi\n",
    "    poste = soup.select_one('li.list-group-item:-soup-contains(\"Poste\")').strong.next_sibling.strip()\n",
    "    type_offre = soup.select_one('li.list-group-item:-soup-contains(\"Type d\\'offre\")').strong.next_sibling.strip()\n",
    "    metiers = soup.select_one('li.list-group-item:-soup-contains(\"Métier(s):\")').strong.next_sibling.strip()\n",
    "    niveaux = soup.select_one('li.list-group-item:-soup-contains(\"Niveau(x):\")').strong.next_sibling.strip()\n",
    "    experience = soup.select_one('li.list-group-item:-soup-contains(\"Expérience:\")').strong.next_sibling.strip()\n",
    "    lieu = soup.select_one('li.list-group-item:-soup-contains(\"Lieu:\")').strong.next_sibling.strip()\n",
    "    \n",
    "    # Extraction des dates de publication et de limite\n",
    "    date_publication = soup.find('strong', string='Date de publication:').find_next('span').text.strip()\n",
    "    date_limite = soup.find('strong', string='Date limite:').find_next('span').text.strip()\n",
    "    \n",
    "   # description = soup.select_one('div.text-col.post.small-post.col-md-9.col-xs-12 ul.list-group').text.strip()\n",
    "    description = soup.select_one('div.entry-content').text.strip()\n",
    "\n",
    "    return {\n",
    "        \"Poste\": [poste],\n",
    "        \"Type d'offre\": [type_offre],\n",
    "        \"Métier(s)\": [metiers],\n",
    "        \"Niveau(x)\": [niveaux],\n",
    "        \"Expérience\": [experience],\n",
    "        \"Lieu\": [lieu],\n",
    "        \"Date de publication\": [date_publication],\n",
    "        \"Date limite\": [date_limite],\n",
    "        \"Description\": [description]\n",
    "    }\n",
    "\n",
    "# Liste des URLs à scraper\n",
    "urls = list(df_projobivoire['EmailURL'])\n",
    "\n",
    "# Liste pour stocker les DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Boucle sur chaque URL\n",
    "for url in urls:\n",
    "    # Envoyer une requête GET au site\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Vérifier si la requête a réussi (statut 200)\n",
    "    if response.status_code == 200:\n",
    "        # Analyser le contenu de la page avec BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            # Extract job information\n",
    "            job_info = extract_job_information(soup)\n",
    "\n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(job_info)\n",
    "\n",
    "            # Ajouter le DataFrame à la liste\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    else:\n",
    "        print(f\"Échec de la requête pour l'URL {url}. Statut : {response.status_code}\")\n",
    "\n",
    "# Concaténer tous les DataFrames en un seul DataFrame\n",
    "df_E = pd.concat(dfs, ignore_index=True)\n",
    "df_E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les listes existantes en tant que colonnes au DataFrame\n",
    "df_E['Title'] = list(df_projobivoire['Title'])\n",
    "df_E['Type'] = list(df_projobivoire['Type'])\n",
    "df_E['DatePosted'] =  list(df_projobivoire['DatePosted'])\n",
    "df_E['DateClosing'] =  list(df_projobivoire['DateClosing'])\n",
    "df_E['Category'] =  list(df_projobivoire['Category'])\n",
    "df_E['EmailURL'] =  list(df_projobivoire['EmailURL'])\n",
    "\n",
    "# Réorganiser les colonnes selon vos besoins\n",
    "\n",
    "df_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportez le DataFrame en fichier CSV\n",
    "df_projobivoire.to_csv(\"df_projobivoire.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53123abd",
   "metadata": {},
   "source": [
    "# Emploi_jeune.ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://agenceemploijeunes.ci/site/offres-emplois?page={}\".format(category) for category in range(17)\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Utilisation d'un en-tête pour éviter d'être bloqué\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Lists to store data\n",
    "job_titles = []\n",
    "publication_dates = []\n",
    "application_deadlines = []\n",
    "locations = []\n",
    "job_descriptions = []\n",
    "job_types = []\n",
    "diploma_requirements = []\n",
    "\n",
    "# Loop through each URL\n",
    "for url in urls:\n",
    "    # Send a request to the website\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    # Find job listings\n",
    "    job_listings = soup.find_all('div', class_='post-bx')\n",
    "\n",
    "    # Extract data from each job listing\n",
    "    for job_listing in job_listings:\n",
    "    # Job title\n",
    "        job_title = job_listing.find('h4').text.strip()\n",
    "        job_titles.append(job_title)\n",
    "\n",
    "    # Publication date and application deadline\n",
    "        date_info = job_listing.find_all('li', {'class': ''})\n",
    "        if date_info:\n",
    "            publication_date = date_info[0].text.replace('Publié le:', '').strip()\n",
    "            application_deadline = date_info[1].text.replace('Date limite:', '').strip()\n",
    "            publication_dates.append(publication_date)\n",
    "            application_deadlines.append(application_deadline)\n",
    "\n",
    "    # Location\n",
    "        location = date_info[2].text.replace('ABENGOUROU', '').strip()\n",
    "        locations.append(location)\n",
    "\n",
    "    # Job description\n",
    "        job_description = job_listing.find('p').text.strip()\n",
    "        job_descriptions.append(job_description)\n",
    "\n",
    "    # Job type\n",
    "        job_type = job_listing.find('span', {'class': 'pull-right'}).text.strip()\n",
    "        job_types.append(job_type)\n",
    "\n",
    "    # Diploma requirement\n",
    "        diploma_requirement = job_listing.find('div', {'class': 'salary-bx'}).text.strip()\n",
    "        diploma_requirements.append(diploma_requirement)\n",
    "\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "data = {\n",
    "    'Job Title': job_titles,\n",
    "    'Publication Date': publication_dates,\n",
    "    'Application Deadline': application_deadlines,\n",
    "    'Location': locations,\n",
    "    'Job Description': job_descriptions,\n",
    "    'Job Type': job_types,\n",
    "    'Diploma Requirement': diploma_requirements\n",
    "}\n",
    "\n",
    "df_agenceemploijeunes = pd.DataFrame(data)\n",
    "# Utiliser la méthode str.extract pour extraire la valeur après \"Diplôme :\"\n",
    "df_agenceemploijeunes['Diplome'] = df_agenceemploijeunes['Diploma Requirement'].str.extract(r'Diplôme :[ \\t]*([^\\n\\r]*)')\n",
    "\n",
    "\n",
    "# Print or further process the DataFrame\n",
    "df_agenceemploijeunes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cfb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# Liste des liens\n",
    "\n",
    "# En-tête pour éviter d'être bloqué\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Pour exécuter le navigateur en arrière-plan\n",
    "options.add_argument(\"--disable-gpu\")  # Désactiver l'accélération GPU en mode headless\n",
    "chrome_driver_path = \"C:\\\\Users\\\\ngora\\\\OneDrive\\\\Bureau\\\\INS_DATA\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "options.binary_location = \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"  # Remplacez par l'emplacement réel de votre Chrome binary\n",
    "options.add_argument(f\"webdriver.chrome.driver={chrome_driver_path}\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Liste pour stocker les détails de chaque emploi\n",
    "all_job_details = []\n",
    "\n",
    "# Parcourir les liens\n",
    "for url in urls:\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    time.sleep(5)  # Attendre 5 secondes avant la prochaine requête\n",
    "\n",
    "    offres = soup.find_all('div', class_=\"col-xl-9 col-lg-8 col-md-7\")\n",
    "\n",
    "    # Parcourir les offres d'emploi sur la page principale\n",
    "    for offre in offres:\n",
    "        # Trouver la balise <h4> dans la structure HTML pour extraire le lien\n",
    "        offre_link_tag = offre.find('h4')\n",
    "\n",
    "        # Vérifier si la balise <h4> a été trouvée\n",
    "        if offre_link_tag:\n",
    "            # Extraire le lien de l'attribut 'href'\n",
    "            offre_link = offre_link_tag.find('a')['href']\n",
    "            all_job_details.append({'Offre_Link': offre_link})\n",
    "\n",
    "# Fermer le pilote Selenium à la fin\n",
    "driver.quit()\n",
    "\n",
    "# Concaténer tous les détails des emplois en un seul DataFrame\n",
    "if all_job_details:\n",
    "    all_job_details_df = pd.DataFrame(all_job_details)\n",
    "    # Afficher le DataFrame\n",
    "    #print(all_job_details_df)\n",
    "else:\n",
    "    print(\"Aucun détail d'offre d'emploi trouvé.\")\n",
    "    \n",
    "all_job_details_df    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f41f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = list(all_job_details_df[\"Offre_Link\"])\n",
    "# Utilisation d'un en-tête pour éviter d'être bloqué\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Lists to store data\n",
    "job_titles = []\n",
    "locations = []\n",
    "references = []\n",
    "number_of_positions = []\n",
    "closing_dates = []\n",
    "diplomas = []\n",
    "job_types = []\n",
    "experiences = []\n",
    "education_levels = []\n",
    "genders = []\n",
    "job_descriptions = []\n",
    "\n",
    "# Loop through each URL\n",
    "for url in urls:\n",
    "    # Send a request to the website\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    offre_url = url\n",
    "\n",
    "    # Extract job details\n",
    "    job_details = soup.find('div', class_='widget_getintuch')\n",
    "\n",
    "    if job_details:\n",
    "        # Extract data from job details\n",
    "        ul_element = job_details.find('ul')\n",
    "        if ul_element:\n",
    "            details_list = ul_element.find_all('li')\n",
    "\n",
    "            # Iterate through details\n",
    "            for detail in details_list:\n",
    "                label = detail.find('strong')\n",
    "                value_span = detail.find('span', class_='text-black-light')\n",
    "\n",
    "                if label and value_span:\n",
    "                    label_text = label.text.strip()\n",
    "                    value_text = value_span.text.strip()\n",
    "\n",
    "                    if 'Lieu de travail' in label_text:\n",
    "                        locations.append(value_text)\n",
    "                    elif 'Reference' in label_text:\n",
    "                        references.append(value_text)\n",
    "                    elif 'Nombre de poste' in label_text:\n",
    "                        number_of_positions.append(value_text)\n",
    "                    elif 'Date de clôture' in label_text:\n",
    "                        closing_dates.append(value_text)\n",
    "                    elif 'Diplôme' in label_text:\n",
    "                        diplomas.append(value_text)\n",
    "                    elif 'Type de contrat' in label_text:\n",
    "                        job_types.append(value_text)\n",
    "                    elif 'Expérience professionnelle' in label_text:\n",
    "                        experiences.append(value_text)\n",
    "                    elif 'Niveau d\\'études' in label_text:\n",
    "                        education_levels.append(value_text)\n",
    "                    elif 'Sexe' in label_text:\n",
    "                        genders.append(value_text)\n",
    "\n",
    "        # Extract job title and description\n",
    "        job_title_element = soup.find('h3', {'class': 'title-head'})\n",
    "        if job_title_element:\n",
    "            job_title = job_title_element.text.strip()\n",
    "            job_titles.append(job_title)\n",
    "\n",
    "            job_description_info = soup.find('div', {'class': 'job-info-box'}).find('ul')\n",
    "            if job_description_info:\n",
    "                job_description_text = '\\n'.join([li.text.strip() for li in job_description_info.find_all('li')])\n",
    "                job_descriptions.append(job_description_text)\n",
    "            else:\n",
    "                job_descriptions.append(None)\n",
    "        else:\n",
    "            job_titles.append(None)\n",
    "            job_descriptions.append(None)\n",
    "    else:\n",
    "        job_titles.append(None)\n",
    "        job_descriptions.append(None)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "data = {\n",
    "    'offre_url':offre_url,\n",
    "    'Job Title': job_titles,\n",
    "    'Location': locations,\n",
    "    'Reference': references,\n",
    "    'Number of Positions': number_of_positions,\n",
    "    'Closing Date': closing_dates,\n",
    "    'Diploma': diplomas,\n",
    "    'Job Type': job_types,\n",
    "    'Experience': experiences,\n",
    "    'Education Level': education_levels,\n",
    "    'Gender': genders,\n",
    "    'Job Description': job_descriptions\n",
    "}\n",
    "\n",
    "df_job_details = pd.DataFrame(data)\n",
    "\n",
    "# Print or further process the DataFrame\n",
    "df_job_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les listes existantes en tant que colonnes au DataFrame\n",
    "df_job_details['Job Title'] = list(df_agenceemploijeunes['Job Title'])\n",
    "df_job_details['Publication Date'] = list(df_agenceemploijeunes['Type'])\n",
    "df_job_details['Application Deadline'] =  list(df_agenceemploijeunes['Application Deadline'])\n",
    "df_job_details[ 'Location'] =  list(df_agenceemploijeunes[ 'Location'])\n",
    "df_job_details['Job Description'] =  list(df_agenceemploijeunes['Job Description'])\n",
    "df_job_details['Job Type'] =  list(df_agenceemploijeunes['Job Type'])\n",
    "df_job_details['Diploma Requirement'] =  list(df_agenceemploijeunes['Diploma Requirement'])\n",
    "\n",
    "# Réorganiser les colonnes selon vos besoins\n",
    "\n",
    "\n",
    "df_job_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportez le DataFrame en fichier CSV\n",
    "df_job_details.to_csv(\"df_agenceemploijeunes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733e95d",
   "metadata": {},
   "source": [
    "# Alerteemploi.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5dcc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Liste des URLs des offres d'emploi\n",
    "job_listing_urls = [\n",
    "    \"https://alerteemploi.net/toutes-les-offres/\",\n",
    "    # Ajoutez d'autres URLs au besoin\n",
    "]\n",
    "\n",
    "# Configurez Selenium pour s'exécuter en mode headless (sans ouvrir de fenêtre de navigateur)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Liste pour stocker les données des offres d'emploi\n",
    "job_data = []\n",
    "\n",
    "# Boucle à travers chaque URL d'offre d'emploi\n",
    "for url in job_listing_urls:\n",
    "    # Envoyez une requête GET en utilisant Selenium\n",
    "    driver.get(url)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Cliquez sur le bouton \"Load more listings\"\n",
    "            load_more_button = driver.find_element(By.CLASS_NAME, 'load_more_jobs')\n",
    "            load_more_button.click()\n",
    "\n",
    "            # Attendez le chargement des nouvelles offres d'emploi\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.invisibility_of_element_located((By.CLASS_NAME, 'loading_jobs'))\n",
    "            )\n",
    "\n",
    "            # Obtenez le code source de la page mis à jour\n",
    "            page_source = driver.page_source\n",
    "\n",
    "            # Utilisez BeautifulSoup pour analyser le HTML\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Trouvez toutes les offres d'emploi sur la page\n",
    "            job_listings = soup.find_all('li', class_='job_listing')\n",
    "\n",
    "            # Boucle à travers chaque offre d'emploi et extrayez les informations\n",
    "            for listing in job_listings:\n",
    "                job_title = listing.find('div', class_='position').find('h3').text.strip()\n",
    "                company = listing.find('div', class_='company').find('strong').text.strip()\n",
    "                location = listing.find('div', class_='location').text.strip()\n",
    "                date_posted_element = listing.find('li', class_='date').find('time')\n",
    "                date_posted = date_posted_element['datetime'].strip() if date_posted_element else None\n",
    "                listing_url = listing.find('a')['href']\n",
    "\n",
    "                # Stockez les données dans un dictionnaire\n",
    "                job_entry = {\n",
    "                    'Titre du poste': job_title,\n",
    "                    'Entreprise': company,\n",
    "                    'Lieu': location,\n",
    "                    'Date de publication': date_posted,\n",
    "                    'URL': listing_url\n",
    "                }\n",
    "\n",
    "                job_data.append(job_entry)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Sortez de la boucle si le bouton n'est pas trouvé ou s'il y a une exception\n",
    "            break\n",
    "\n",
    "# Fermez le pilote Selenium\n",
    "driver.quit()\n",
    "\n",
    "# Créez un DataFrame\n",
    "df_alerteemploi = pd.DataFrame(job_data)\n",
    "\n",
    "# Affichez le DataFrame\n",
    "df_alerteemploi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# Liste d'URLs des pages d'offres d'emploi\n",
    "urls = list(df_alerteemploi[\"URL\"])\n",
    "# Initialiser une liste pour stocker les données\n",
    "job_data = []\n",
    "\n",
    "# Configurer Selenium pour s'exécuter en mode headless (sans ouvrir de fenêtre de navigateur)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "# Boucle à travers chaque URL\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Initialiser le pilote Selenium\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        # Envoyer une requête GET en utilisant Selenium\n",
    "        driver.get(url)\n",
    "\n",
    "        # Attendre quelques secondes (ajustez selon les besoins)\n",
    "        time.sleep(30)\n",
    "\n",
    "        # Récupérer le code source de la page après l'exécution de JavaScript\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Fermer le pilote Selenium\n",
    "        driver.quit()\n",
    "\n",
    "        # Utiliser BeautifulSoup pour analyser le HTML\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Extraire les détails de l'offre d'emploi\n",
    "        job_title_element = soup.find('h1', class_='entry-title')\n",
    "        job_title = job_title_element.text.strip() if job_title_element else None\n",
    "\n",
    "        # Check if the initial element is found before attempting to find the nested element\n",
    "        author_container = soup.find('div', class_='td-post-author-name')\n",
    "        author_element = author_container.find('a') if author_container else None\n",
    "        author = author_element.text.strip() if author_element else None\n",
    "\n",
    "        date_posted_element = soup.find('time', class_='entry-date')\n",
    "        date_posted = date_posted_element['datetime'].strip() if date_posted_element else None\n",
    "\n",
    "        views_element = soup.find('div', class_='td-post-views')\n",
    "        views = views_element.find('span', class_='td-nr-views-19100').text.strip() if views_element and views_element.find('span', class_='td-nr-views-19100') else None\n",
    "\n",
    "        # Check if the element is found before accessing its properties\n",
    "        image_url_element = soup.find('div', class_='td-post-featured-image')\n",
    "        image_url = image_url_element.find('img')['src'] if image_url_element and image_url_element.find('img') else None\n",
    "\n",
    "        # Ajouter les détails à la liste\n",
    "        job_data.append({\n",
    "            'Job Title': job_title,\n",
    "            'Author': author,\n",
    "            'Date Posted': date_posted,\n",
    "            'Views': views,\n",
    "            'Image URL': image_url,\n",
    "            'Link URL': url  # Ajoutez l'URL de la page à la liste\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite pour l'URL {url}: {str(e)}\")\n",
    "\n",
    "# Créer un DataFrame avec les données extraites\n",
    "\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les listes existantes en tant que colonnes au DataFrame\n",
    "df['Titre du poste'] = list(df_alerteemploi['Titre du poste'])\n",
    "df['Entreprise'] = list(df_alerteemploi['Entreprise'])\n",
    "df['Lieu'] =  list(df_alerteemploi['Lieu'])\n",
    "df[ 'Date de publication'] =  list(df_alerteemploi['Date de publication'])\n",
    "df['URL'] =  list(df_alerteemploi['URL'])\n",
    "\n",
    "# Réorganiser les colonnes selon vos besoins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_job_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68905d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportez le DataFrame en fichier CSV\n",
    "df.to_csv(\"df_alerteemploi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e8bcd",
   "metadata": {},
   "source": [
    "# jobcenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a71b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Liste des URLs des pages d'offres d'emploi\n",
    "urls = [\n",
    "    \"https://rmo-jobcenter.com/fr/nos-offres-emploi.html\",\n",
    "    # Ajoutez d'autres URLs au besoin\n",
    "]\n",
    "\n",
    "# Configurez Selenium pour s'exécuter en mode headless\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "# Liste pour stocker les données des offres d'emploi\n",
    "all_job_data = []\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = \"https://rmo-jobcenter.com\"\n",
    "\n",
    "# Boucle à travers chaque URL\n",
    "for url in urls:\n",
    "    # Initialisez le pilote Selenium\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # Chargez la page avec Selenium\n",
    "    driver.get(url)\n",
    "\n",
    "    # Récupérez le code source de la page après l'exécution du JavaScript\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Fermez le pilote Selenium\n",
    "    driver.quit()\n",
    "\n",
    "    # Utilisez BeautifulSoup pour analyser le HTML\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Trouvez la table contenant les offres d'emploi\n",
    "    table = soup.find('table', class_='liste')\n",
    "\n",
    "    # Liste pour stocker les données des offres d'emploi pour une URL spécifique\n",
    "    job_data = []\n",
    "\n",
    "    # Boucle à travers chaque ligne de la table (sauf la première qui contient les en-têtes)\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        # Extrayez les données de chaque colonne\n",
    "        columns = row.find_all('td')\n",
    "        date = columns[0].text.strip()\n",
    "        filiale = columns[1].text.strip()\n",
    "        fonction = columns[2].text.strip()\n",
    "        secteur = columns[3].text.strip()\n",
    "        reference = columns[4].text.strip()\n",
    "        details_url = columns[5].find('a')['href']\n",
    "\n",
    "        # Rendez l'URL absolue en la combinant avec l'URL de base.\n",
    "        absolute_url = f\"{base_url}/{details_url}\"\n",
    "\n",
    "        # Stockez les données dans un dictionnaire\n",
    "        job_entry = {\n",
    "            'Date': date,\n",
    "            'Filiale': filiale,\n",
    "            'Fonction': fonction,\n",
    "            'Secteur': secteur,\n",
    "            'Référence / Statut': reference,\n",
    "            'Détails URL': absolute_url\n",
    "        }\n",
    "\n",
    "        job_data.append(job_entry)\n",
    "\n",
    "    # Ajoutez les données de cette URL à la liste globale\n",
    "    all_job_data.extend(job_data)\n",
    "\n",
    "# Créez un DataFrame avec toutes les données extraites\n",
    "df_jobcenter = pd.DataFrame(all_job_data)\n",
    "\n",
    "# Affichez le DataFrame\n",
    "df_jobcenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f407566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Liste des URLs des pages d'offres d'emploi\n",
    "urls = list(df_jobcenter['Détails URL'])\n",
    "\n",
    "# Configurez Selenium pour s'exécuter en mode headless\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "# Liste pour stocker les données des offres d'emploi\n",
    "all_job_data = []\n",
    "\n",
    "# Boucle à travers chaque URL\n",
    "for url in urls:\n",
    "    # Initialisez le pilote Selenium\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # Chargez la page avec Selenium\n",
    "    driver.get(url)\n",
    "\n",
    "    # Récupérez le code source de la page après l'exécution du JavaScript\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Fermez le pilote Selenium\n",
    "    driver.quit()\n",
    "\n",
    "    # Utilisez BeautifulSoup pour analyser le HTML\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Trouvez la div contenant les informations détaillées\n",
    "    details_div = soup.find('div', {'id': 'content_articles'})\n",
    "\n",
    "    # Extract details from the div\n",
    "    job_title = details_div.find('div', {'id': 'h2_imprime'}).text.strip()\n",
    "    job_description = details_div.find('div', {'class': 'text-content'}).text.strip()\n",
    "\n",
    "    # Additional details can be extracted similarly\n",
    "\n",
    "    # Stockez les données dans un dictionnaire\n",
    "    job_entry = {\n",
    "        'Job Title': job_title,\n",
    "        'Job Description': job_description,\n",
    "        'Job URL': url\n",
    "        # Add more details as needed\n",
    "    }\n",
    "\n",
    "    # Ajoutez les données de cette URL à la liste globale\n",
    "    all_job_data.append(job_entry)\n",
    "\n",
    "# Créez un DataFrame avec toutes les données extraites\n",
    "df_details = pd.DataFrame(all_job_data)\n",
    "\n",
    "# Affichez le DataFrame\n",
    "df_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e57d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les listes existantes en tant que colonnes au DataFrame\n",
    "df_jobcenter['Job Title'] = list(df_details['Job Title'])\n",
    "df_jobcenter['Job Description'] = list(df_details['Job Description'])\n",
    "df_jobcenter['Job URL'] =  list(df_details['Job URL'])\n",
    "\n",
    "# Réorganiser les colonnes selon vos besoins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_job_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b386658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportez le DataFrame en fichier CSV\n",
    "df_jobcenter.to_csv(\"df_jobcenter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed5596",
   "metadata": {},
   "source": [
    "# yop.l-frii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# List of URLs\n",
    "urls = [\n",
    "    \"https://yop.l-frii.com/offres-demplois/{}/\".format(category) for category in range(3276)\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "job_data = []\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    job_articles = soup.find_all('article', class_='type-emploi')\n",
    "\n",
    "    for article in job_articles:\n",
    "        # Check if the 'h2' element with class 'elementor-heading-title' is found\n",
    "        job_title_element = article.find('h2', class_='elementor-heading-title')\n",
    "        job_title = job_title_element.text.strip() if job_title_element else None\n",
    "\n",
    "        job_link = article.find('a', href=True)['href']\n",
    "        \n",
    "        # Check if the image element is found\n",
    "        job_image_element = article.find('img', class_='attachment-large')\n",
    "        job_image = job_image_element['src'] if job_image_element else None\n",
    "\n",
    "        job_data.append({\n",
    "            \"Job Title\": job_title,\n",
    "            \"Job Link\": job_link,\n",
    "            \"Job Image\": job_image,\n",
    "            \"Source URL\": url  # Include the source URL in the DataFrame\n",
    "        })\n",
    "\n",
    "df_yop_l_frii = pd.DataFrame(job_data)\n",
    "df_yop_l_frii\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83866c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportez le DataFrame en fichier CSV\n",
    "df_yop_l_frii.to_csv(\"df_yop_l_frii.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab9b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "def emploi_ci():\n",
    "    def extract_text(element, tag_name=None):\n",
    "        tag = element.find(tag_name)\n",
    "        return tag.text.strip() if tag else \"\"\n",
    "\n",
    "    def clean_text(text):\n",
    "        return text.replace('D\\x92', ' ').replace('d\\x92', ' ').replace('\\x92', ' ').replace('\\r\\n', '').replace('\\xa0', '')\n",
    "\n",
    "    def scrape_emploi_ci(url):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=500)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur de connexion à {url} : {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        job_description_wrappers = soup.find_all('div', class_='job-description-wrapper')\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for wrapper in job_description_wrappers:\n",
    "            h5_tag = wrapper.find('h5')\n",
    "            poste = extract_text(h5_tag, 'a')\n",
    "\n",
    "            job_recruiter_tag = wrapper.find('p', class_='job-recruiter')\n",
    "            date_and_company = job_recruiter_tag.text.strip().split('|')\n",
    "            date = date_and_company[0].strip() if date_and_company else \"\"\n",
    "            entreprise = extract_text(job_recruiter_tag, 'a')\n",
    "\n",
    "            description_tag = wrapper.find('div', class_='search-description')\n",
    "            description = clean_text(description_tag.text.strip()) if description_tag else \"\"\n",
    "\n",
    "            region_tag = wrapper.find('p', text='Région de :')\n",
    "            region = extract_text(region_tag) if region_tag else \"\"\n",
    "\n",
    "            data_list.append({\n",
    "                'Poste': poste,\n",
    "                'Entreprise': entreprise,\n",
    "                'Date': date,\n",
    "                'Description': description,\n",
    "                'Région': region,\n",
    "                'URL' : url\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(data_list)\n",
    "        return df\n",
    "\n",
    "    # Liste des liens\n",
    "    categories = [\"31\", \"1127\", \"29\", \"37\", \"1115\", \"30\", \"1115\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"39\", \"38\", \"40\", \"525\", \"41\", \"28\"]\n",
    "    #categories=[\"31\"]\n",
    "    # Liste d'URLs générées\n",
    "    urls = [\"https://www.emploi.ci/recherche-jobs-cote-ivoire/?f%5B0%5D=im_field_offre_metiers%3A{}\".format(category) for category in categories]\n",
    "\n",
    "    # Créer un DataFrame à partir des liens\n",
    "    df = pd.concat([scrape_emploi_ci(url) for url in urls], ignore_index=True)\n",
    "\n",
    "\n",
    "    from requests.exceptions import ChunkedEncodingError, ConnectionError, ReadTimeout\n",
    "\n",
    "    # Liste des liens\n",
    "\n",
    "    # En-tête pour éviter d'être bloqué\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Pour exécuter le navigateur en arrière-plan\n",
    "    options.add_argument(\"--disable-gpu\")  # Désactiver l'accélération GPU en mode headless\n",
    "    chrome_driver_path = \"C:\\\\Users\\\\ngora\\\\OneDrive\\\\Bureau\\\\INS_DATA\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"  # Remplacez par l'emplacement réel de votre Chrome binary\n",
    "    options.add_argument(f\"webdriver.chrome.driver={chrome_driver_path}\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Liste pour stocker les détails de chaque emploi\n",
    "    all_job_details = []\n",
    "\n",
    "    # Parcourir les liens\n",
    "    for url in list(df[\"URL\"]):\n",
    "        req = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        time.sleep(5)  # Attendre 5 secondes avant la prochaine requête\n",
    "\n",
    "        offres = soup.find_all('div', class_=\"job-description-wrapper\")\n",
    "\n",
    "        # Parcourir les offres d'emploi sur la page principale\n",
    "        for offre in offres:\n",
    "            # Trouver la balise <h4> dans la structure HTML pour extraire le lien\n",
    "            offre_link_tag = offre.find('h5')\n",
    "\n",
    "            # Vérifier si la balise <h4> a été trouvée\n",
    "            if offre_link_tag:\n",
    "                # Extraire le lien de l'attribut 'href'\n",
    "                offre_link = offre_link_tag.find('a')['href']\n",
    "                all_job_details.append({'Offre_Link': \"https://www.emploi.ci\"+offre_link, 'URL' :url})\n",
    "\n",
    "    # Fermer le pilote Selenium à la fin\n",
    "    driver.quit()\n",
    "\n",
    "    # Concaténer tous les détails des emplois en un seul DataFrame\n",
    "    if all_job_details:\n",
    "        all_job_details_df = pd.DataFrame(all_job_details)\n",
    "        # Afficher le DataFrame\n",
    "        #print(all_job_details_df)\n",
    "    else:\n",
    "        print(\"Aucun détail d'offre d'emploi trouvé.\")\n",
    "\n",
    "    # Fusionner les deux DataFrames sur la colonne 'URL'\n",
    "    emploi_df = pd.merge(df, all_job_details_df, on='URL')\n",
    "    emploi_df = emploi_df.drop_duplicates()\n",
    "    \n",
    "    #df[\"URL\"]=list(all_job_details_df[\"URL\"])\n",
    "    #df[\"Offre_Link\"]=list(all_job_details_df[\"Offre_Link\"])\n",
    "\n",
    "    # Fonction pour extraire les informations d'une page\n",
    "    def extract_information(url):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=120)  # Augmentation du délai à 20 secondes\n",
    "            response.raise_for_status()\n",
    "            response.encoding = 'utf-8'\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extraction des informations sur l'entreprise\n",
    "            company_info = soup.select_one('.job-ad-company')\n",
    "            entreprise = {\n",
    "                \"Offre_Link\" : url,\n",
    "                'Nom': company_info.select_one('.company-title a').text.strip() if company_info and company_info.select_one('.company-title a') else None,\n",
    "                'Secteur d´activité': ', '.join(item.text.strip() for item in company_info.select('.sector-title .field-item')) if company_info and company_info.select('.sector-title .field-item') else None,\n",
    "                'Description de l\\'entreprise': soup.select_one('.job-ad-company-description label + *').text.strip() if soup.select_one('.job-ad-company-description label + *') else None\n",
    "            }\n",
    "\n",
    "            # Extraction des informations sur l'annonce\n",
    "            annonce_info = soup.select_one('.job-ad-details')\n",
    "            annonce = {\n",
    "                'Poste': soup.select_one('.ad-ss-title').text.strip() if soup.select_one('.ad-ss-title') else None,\n",
    "                'Missions': [li.text.strip() for li in soup.select('.content ul.missions li')] if soup.select('.content ul.missions') else None,\n",
    "                'Profil recherché': [li.text.strip() for li in soup.select('.content ul.profil li')] if soup.select('.content ul.profil') else None,\n",
    "                'Métier': soup.select_one('.job-ad-criteria .field-name-field-offre-metiers .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-metiers .field-item') else None,\n",
    "                'Secteur d´activité (de l\\'annonce)': soup.select_one('.job-ad-criteria .field-name-field-offre-secteur .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-secteur .field-item') else None,\n",
    "                'Type de contrat': soup.select_one('.job-ad-criteria .field-name-field-offre-contrat-type .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-contrat-type .field-item') else None,\n",
    "                'Région': soup.select_one('.job-ad-criteria .field-name-field-offre-region .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-region .field-item') else None,\n",
    "                'Ville': soup.select_one('.job-ad-criteria .field-name-field-offre-ville .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-ville .field-item') else None,\n",
    "                'Niveau d\\'expérience': soup.select_one('.job-ad-criteria .field-name-field-offre-niveau-experience .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-niveau-experience .field-item') else None,\n",
    "                'Niveau d\\'études': soup.select_one('.job-ad-criteria .field-name-field-offre-niveau-etude .field-item').text.strip() if soup.select_one('.job-ad-criteria .field-name-field-offre-niveau-etude .field-item') else None,\n",
    "                'Compétences clés': [li.text.strip() for li in soup.select('.job-ad-criteria .field-name-field-offre-tags .field-item')] if soup.select('.job-ad-criteria .field-name-field-offre-tags .field-item') else None,\n",
    "                'Nombre de poste(s)': soup.select_one('.job-ad-criteria td:contains(\"Nombre de poste(s) :\") + td').text.strip() if soup.select_one('.job-ad-criteria td:contains(\"Nombre de poste(s) :\") + td') else None,\n",
    "            }\n",
    "\n",
    "            return {'entreprise': entreprise, 'annonce': annonce}\n",
    "\n",
    "        except (ConnectionError, ReadTimeout, ChunkedEncodingError) as e:\n",
    "            print(f\"Erreur lors de la requête {url}: {e}\")\n",
    "            # Relancer la requête\n",
    "            entreprise = {\n",
    "                \"Offre_Link\" : url,\n",
    "                'Nom': \"\",\n",
    "                'Secteur d´activité': \"\",\n",
    "                'Description de l\\'entreprise':\"\"}\n",
    "            annonce = {'Poste':\"\",\n",
    "                       'Missions': \"\",\n",
    "                       'Profil recherché':\"\",\n",
    "                       'Métier':\"\",\n",
    "                       'Secteur d´activité (de l\\'annonce)':\"\",\n",
    "                       'Type de contrat':\"\",\n",
    "                       'Région': \"\",\n",
    "                       'Ville':\"\",\n",
    "                       'Niveau d\\'expérience':\"\",\n",
    "                       'Niveau d\\'études':\"\",\n",
    "                       'Compétences clés':\"\",\n",
    "                       'Nombre de poste(s)':\"\"}\n",
    "\n",
    "\n",
    "            return {'entreprise': entreprise, 'annonce': annonce}\n",
    "\n",
    "    # Liste des URLs\n",
    "    urls = list(emploi_df['Offre_Link'])\n",
    "\n",
    "    # Initialisation d'une liste pour stocker les DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # Boucle à travers chaque URL\n",
    "    for url in urls:\n",
    "        data = extract_information(url)\n",
    "\n",
    "        # Si la requête a échoué, passez à l'URL suivante\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        # Création du DataFrame pour chaque URL\n",
    "        df = pd.DataFrame([data['entreprise'] | data['annonce']])\n",
    "\n",
    "        # Ajout du DataFrame à la liste\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concaténation des DataFrames de chaque URL\n",
    "    result_df = pd.concat(df_list, ignore_index=True)\n",
    "    # Ajouter les listes existantes en tant que colonnes au DataFrame\n",
    "    result_df = pd.merge(result_df, emploi_df, on='Offre_Link')\n",
    "    result_df = emploi_df.drop_duplicates()\n",
    "\n",
    "\n",
    "# Réorganiser les colonnes selon vos besoins\n",
    "#Poste \tEntreprise \tDate \tDescription \tRégion \tURL \tOffre_Link\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Appel de la fonction pour obtenir le DataFrame\n",
    "emploi_df=emploi_ci()\n",
    "emploi_df.reset_index(drop=True, inplace=True)\n",
    "emploi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab399ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8988d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
